# Comparing multiple means

Throughout this lab, we will provide a pipeline to help you wrangle data, perform statistical analyses, and (perhaps most importantly) visualize data in R. Here, we will learn how to compare the means using parametric tests and medians using non-parametric tests of multiple groups.

Let's load the usual packages, and one new package called `car` (Companion to Applied Regression).

```{r, message=F, warning=F}
library(gapminder)
library(tidyverse)
library(car)  # for Levene's test
```


## One-way ANOVA

### The iris dataset

The iris dataset contains information about three species of flowers: setosa, veriscolor, and virginia. Iris is a built-in dataset, meaning we can call it without reading it in.

+ `iris$Species` refers to one column in `iris`. That is, the column with the name of the species (setosa, versicolor, or virginica).
+ We can see how many rows and columns are in a `data.frame` with the `dim` command. `dim(iris)` prints out the number of rows (`r nrow(iris)`) and the number of columns (`r ncol(iris)`):

```{r}
head(iris)
summary(iris)
```

Analysis of Variance (ANOVA) allows us to test whether there are differences in the mean between multiple samples. The question we will address is:

**Are there differences in average sepal width among the three species?**

To run an ANOVA, we need to check if

  1. The variance is is equal for *each group*, and
  2. The data distributes normally within *each group*.

Let's address the first point.

```{r}
leveneTest(Sepal.Width ~ Species, data = iris)
```

A p-value of 0.5555 suggested that the variances are not significantly different. This means we should proceed with a parametric test like ANOVA (otherwise, use the Kruskal-Wallis test). Keep in mind we haven't yet checked the normality. We will do it after running ANOVA.

We start by building an analysis of variance model with the `aov()` function:

In this case, we pass _two_ arguments to the `aov()` function:

1. For the `formula` parameter, we pass `Sepal.Width ~ Species`. This format is used throughout R for describing relationships we are testing. The format is `y ~ x`, where the response variables (e.g. `y`) are to the left of the tilde (~) and the predictor variables (e.g. `x`) are to the right of the tilde. In this example, we are asking if petal length is significantly different among the three species.
2. We also need to tell R where to find the `Sepal.Width` and `Species` data, so we pass the variable name of the `iris data.frame` to the `data` parameter.  

But we want to store the model, not just print it to the screen, so we use the assignment operator `<-` to store the product of the `aov` function in a variable of our choice

```{r}
Sepal.Width.aov <- aov(formula = Sepal.Width ~ Species, data = iris)
```

Notice how when we execute this command, nothing printed in the console. This is because we instead sent the output of the `aov` call to a variable. If you just type the variable name, you will see the familiar output from the `aov` function:  

```{r}
Sepal.Width.aov
```

To see the results of the ANOVA, we call the `summary()` function:

```{r}
summary(object = Sepal.Width.aov)
```

The species _do_ have significantly different sepal width (P < 0.001). However, ANOVA does not tell us _which_ species are different. We can run a _post hoc_ test to assess _how_ the species are different. A Tukey test comparing means would be one option. We will do the Tukey test after determining normality.

Now, let's take a look at the normality. First, we will plot the diagnostic figures.

```{r}
plot(Sepal.Width.aov)
```

Most importantly, the residuals in Q-Q plot (upper right) should align with the line pretty well. This figure is acceptable. If the residuals deviate from the line too much, the data would not be considered normal. If you still perform the ANOVA, you should view your results critically (or ignore them, at worst).

Please do not include such diagnostic figures in the main text of your manuscripts. This might qualify for a supplementary figure at most.

Although we've also examined residuals with the QQ plot, we can also use a formal test:

```{r}
residuals_Sepal_Width <- residuals(object = Sepal.Width.aov)
shapiro.test(x = residuals_Sepal_Width)
```

A p-value of 0.323 suggested that the assumption of normality is reasonable.

Recall that a residual is an "error" in result. More specifically, a residual is the difference of a given data point from the mean ($r = x - \mu$).

So far, we have demonstrated

  1. Normality in distribution.
  2. Homogeneity variance, and

These two justified our choice for one-way ANOVA. The result of ANOVA also indicated that at least one species of the 3 has significantly different sepal width from others. Which one?

To do this, we need to run "Post-Hoc" test. Let's do Tukey Honest Significant Differences (HSD). The nice thing is that `TukeyHSD()` can directly take the result of ANOVA as the argument.

```{r}
TukeyHSD(Sepal.Width.aov)
```

The difference between every pair are significant ($p < 0.05$).


### Non-parametric alternatives to ANOVA

In reality, your data usually will not fulfill every assumption for an ANOVA.

In case of a non-normal sample, there are two ways to address the problem:

  1. Apply appropriate [data transformations techniques](https://fmwww.bc.edu/repec/bocode/t/transint.html), or
  2. Use a non-parametric test

I highly recommend you to explore the tricks of data transformation. If you can rescue it back to normal distribution, parametric tests usually can allow you to do more powerful analysis.

If you have exhausted your attempts to data transformation, you may then use non-parametric tests. A note for [Kruskal-Wallis test](http://www.biostathandbook.com/kruskalwallis.html).

When your data doesn't satisfy the normality or equal variance assumption, ANOVA does not strictly apply. However, the one-way ANOVA is not very sensitive to deviations from normality. Kruskal-Wallis doesn't assume normality, but it does assume same distribution across groups (equal variance). If your data do not meet either assumption, you would want to use Welch's One-way test. Now, let's get back to gapminder data.

Let's add another categorical variable calle `Income_Level`. This time we will split by the quartiles.

```{r}
dat.1952 <- gapminder %>% filter(year == 1952)
border_1952 <- quantile(dat.1952$gdpPercap, c(.25, .50, .75))
dat.1952$Income_Level_1952 <- cut(dat.1952$gdpPercap,
                                  c(0, border_1952[1], border_1952[2], border_1952[3], Inf),
                                  c('Low', 'Low Middle', 'High Middle', 'High'))
head(dat.1952)

dat.2007 <- gapminder %>% filter(year == 2007)
border_2007 <- quantile(dat.2007$gdpPercap, c(.25, .50, .75))
dat.2007$Income_Level_2007 <- cut(dat.2007$gdpPercap,
                                  c(0, border_2007[1],
                                    border_2007[2], border_2007[3], Inf),
                                  c('Low', 'Low Middle', 'High Middle', 'High'))
head(dat.2007)
```

For now, let's focus on the data of in 1952.

```{r}
ggplot(data = dat.1952, aes(x = Income_Level_1952, y = lifeExp)) +
  geom_boxplot() +
  theme_classic()
```

Let's check the variance. 

```{r}
leveneTest(lifeExp ~ Income_Level_1952, data = dat.1952)
```

A p-value of 0.0047 suggested that the variances are significantly different. Therefore, we shoud not run ANOVA or Kruskal-Wallis. Let's run Welch's one-way test.

```{r}
result <- oneway.test(lifeExp ~ Income_Level_1952, data = dat.1952)
result
```

A p-value of 2.2e-16 suggested that at least one category of `Income_Level_1952` had values of `lifeExp` that are significantly different from others. Let's run a Post-Hoc test to find out.

Since we are running a non-parametric test, an appropriate test would be Games-Howell post-hoc test. Unfortunately, R does not have a built-in function for Games-Howell. Let's define a function to do this task.

**Note**: you don't need to know how the code below works.

```{r}
games.howell <- function(grp, obs) {
  #Create combinations
  combs <- combn(unique(grp), 2)
  
  # Statistics that will be used throughout the calculations:
  # n = sample size of each group
  # groups = number of groups in data
  # Mean = means of each group sample
  # std = variance of each group sample
  n <- tapply(obs, grp, length)
  groups <- length(tapply(obs, grp, length))
  Mean <- tapply(obs, grp, mean)
  std <- tapply(obs, grp, var)
  
  statistics <- lapply(1:ncol(combs), function(x) {
    mean.diff <- Mean[combs[2,x]] - Mean[combs[1,x]]
    # t-values
    t <- abs(Mean[combs[1,x]] - Mean[combs[2,x]]) / sqrt((std[combs[1,x]] / n[combs[1,x]]) + (std[combs[2,x]] / n[combs[2,x]]))
    # Degrees of Freedom
    df <- (std[combs[1,x]] / n[combs[1,x]] + std[combs[2,x]] / n[combs[2,x]])^2 / # numerator dof
      ((std[combs[1,x]] / n[combs[1,x]])^2 / (n[combs[1,x]] - 1) + # Part 1 of denominator dof
         (std[combs[2,x]] / n[combs[2,x]])^2 / (n[combs[2,x]] - 1)) # Part 2 of denominator dof
    # p-values
    p <- ptukey(t * sqrt(2), groups, df, lower.tail = FALSE)
    # sigma standard error
    se <- sqrt(0.5 * (std[combs[1,x]] / n[combs[1,x]] + std[combs[2,x]] / n[combs[2,x]]))
    # Upper Confidence Limit
    upper.conf <- lapply(1:ncol(combs), function(x) {
      mean.diff + qtukey(p = 0.95, nmeans = groups, df = df) * se
    })[[1]]
    # Lower Confidence Limit
    lower.conf <- lapply(1:ncol(combs), function(x) {
      mean.diff - qtukey(p = 0.95, nmeans = groups, df = df) * se
    })[[1]]
    # Group Combinations
    grp.comb <- paste(combs[1,x], ':', combs[2,x])
    # Collect all statistics into list
    stats <- list(grp.comb, mean.diff, se, t, df, p, upper.conf, lower.conf)
  })
  
  # Unlist statistics collected earlier
  stats.unlisted <- lapply(statistics, function(x) {
    unlist(x)
  })
  
  # Create dataframe from flattened list
  results <- data.frame(matrix(unlist(stats.unlisted), nrow = length(stats.unlisted), byrow=TRUE))
  # Select columns set as factors that should be numeric and change with as.numeric
  results[c(2, 3:ncol(results))] <- round(as.numeric(as.matrix(results[c(2, 3:ncol(results))])), digits = 3)
  # Rename data frame columns
  colnames(results) <- c('groups', 'Mean Difference', 'Standard Error', 't', 'df', 'p', 'upper limit', 'lower limit')
  return(results)
}
```

After defining the function, we can use it. If you decide to use the Games-Howell function, you can simply copy-and-paste it.

```{r}
games.howell(grp = dat.1952$Income_Level_1952,  # groups, the categorical variable
             obs = dat.1952$lifeExp)            # observations, the continuous variable
```


## Beyond the one-way ANOVA

As seen in the previous example, a one-way ANOVA is a great tool for comparing multiple groups on one categorical variable such as income level. But what if we wanted to examine if life expectancy differs across the years 1952, 1972, and 2007 in addition to income level? In this case, we will need a two-way ANOVA.

Although we won't cover a two-way ANOVA in this text, you can learn more about it in [this article](http://www.sthda.com/english/wiki/two-way-anova-test-in-r).


<script>
iFrameResize({}, ".interactive");
  
var coll = document.getElementsByClassName("collapsible");
for (var i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.maxHeight){
      content.style.maxHeight = null;
      setTimeout(function() {
        content.style.paddingTop = "0";
        content.style.paddingBottom = "0";
      }, 250)
    } else {
      content.style.paddingTop = "15px";
      content.style.paddingBottom = "10px";
      content.style.maxHeight = content.scrollHeight + "px";
    } 
  });
}
</script>

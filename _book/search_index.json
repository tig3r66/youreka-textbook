[["index.html", "A Gentle Introduction to Data Science with R Explore, analyze, and visualize data Welcome to Youreka Canada How to use this textbook Contributors Credit", " A Gentle Introduction to Data Science with R Explore, analyze, and visualize data Eddie Guo Youreka Canada Programs Team Welcome to Youreka Canada Welcome to the Youreka Canada program! In this course, you will learn how to wrangle data, perform statistical tests, and visualize data with R. The purpose of this textbook is to provide a companion to the Youreka Canada program, which teaches introductory statistics, data science, and research methods. Here, we offer an intuitive approach to data science rather than a rigorous, proof-based course. As such, this textbook does not assume you have any prior knowledge other than basic arithmetic, and it should be accessible to both high school and undergraduate students. How to use this textbook Please note that this text goes into additional detail not covered in session. All optional material is marked as OPTIONAL in the headings. Again, note that this text complements the Youreka program. These notes are not a substitute for attending the Youreka sessions! This text focuses on how the material taught in-session can be applied using the R programming language. As with all things programming, the best way to learn is to actively code. That is, when you read this textbook, open RStudio and play around with the presented code—try to find alternative solutions, or even break the code. Don’t be afraid to make mistakes, and soon enough, you will be confident to code on your own! Contributors This textbook was written by Eddie Guo, Pouria Torabi, Shuce Zhang, and Devin Aggarwal, who are part of the Youreka Canada Programs Department. A special thanks goes to Matthew Pietrosanu for his critical statistical review of the Youreka program. Credit This material was adapted from: Jennifer Bryan STAT 545 at UBC https://stat545.com/ Teacups, Giraffes, &amp; Statistics by Hasse Walum and Desirée De Leon https://tinystats.github.io/teacups-giraffes-and-statistics/ "],["license.html", "License", " License This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. This is a human-readable summary of (and not a substitute for) the license. Please see https://creativecommons.org/licenses/by-sa/4.0/legalcode for the full legal text. You are free to: Share—copy and redistribute the material in any medium or format Remix—remix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: Attribution—You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. ShareAlike—If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original. No additional restrictions—You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. Notices: You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material. "],["install.html", "1 Install R and RStudio 1.1 Installing R 1.2 Installing RStudio 1.3 Add-on packages", " 1 Install R and RStudio Why do we need to install both R and RStudio? Well, the answer is the following: R is the programming language whereas RStudio is the integrated development environment (IDE) for R. You can think of R as the thing that turns your code into commands that your computer runs and RStudio as a fancy text editor (although it is so much more than that!). 1.1 Installing R Go to the R Project website. On the left side bar, click on “CRAN” under “Download.” Choose the mirror you wish to download from (e.g., https://mirror.rcg.sfu.ca/mirror/CRAN/) Download the correct version for your OS. Ensure to download the latest release of R. 1.2 Installing RStudio Go to the RStudio website. Navigate to the RStudio page and download RStudio Desktop. If you have a pre-existing installation of R and/or RStudio, we highly recommend that you update both. If you upgrade R, you’ll need to update any packages you have installed. Type the following command into the Console in RStudio: update.packages(ask = FALSE, checkBuilt = TRUE) Once you’ve installed an updated version of R and RStudio, open RStudio. You should get a window similar to this screenshot, but yours will be more boring because you haven’t written any code or made any figures yet! Place your cursor in the pane called “Console,” which is where you interact with R. Type print('Hello World!') in the console and hit the enter or return key. You should see “Hello World!” print to the screen. If you do, you’ve succeeded in installing R and RStudio. 1.3 Add-on packages R contains a huge number of packages that enhances its functionality. People often share useful code they have developed as a package via CRAN and GitHub. To install a package from CRAN (e.g., the tidyverse), type this into the R console: install.packages(&#39;tidyverse&#39;, dependencies = TRUE) By including dependencies = TRUE, we are including any additional packages our target package requires. Please also install gapminder, ggsignif, and ggpubr. Without further ado, let’s jump into our adventure with R! "],["intro-to-r.html", "2 Intro to R 2.1 Using interactive windows 2.2 Objects 2.3 Functions 2.4 Math operators 2.5 Conditionals 2.6 Working directory", " 2 Intro to R We will begin our adventure by opening RStudio. If this is your first time opening RStudio, you should see the following panes: Console (entire left) Environment/History (upper right) Files/Plots/Packages/Help (lower right) You can change the default location of the panes, among many other things: Customizing RStudio. For now, place your cursor in the console so we can start coding with R! 2.1 Using interactive windows Throughout this text, we will use interactive code blocks provided by the leanrR package. This way, you can run code without having to open RStudio! Below you can see an example of an interactive R window. In the white window (under the button with the text Start Over) is where you will write code to be executed. To run code you have written, click the green Run Code button and observe how information pops up under the window. If no errors were made, this is where the answer will be returned. Error messages are shown in a red box, also under the learnR window. Now spend a few minutes using the window below as a calculator and run simple calculations by clicking Run Code. 2.2 Objects R is an object-oriented programming language. This means R creates different types of objects that we can manipulate with functions and operators. To create an object in R, we can assign a value to an object using an assignment operator using either a left arrow &lt;- or an equal sign =. Click the “Run Code” button to get started and play around with the code! In plain English, the above snippet tells us that “five times ten is assigned to my_object.” By convention, we use &lt;- to assign variables. Don’t be lazy and use = to assign variables. Although it will work, it will just sow confusion later. Code is miserable to read on a good day. Give your eyes a break and use &lt;-. Although object names are flexible, we need to follow some rules: Object names cannot start with a digit and cannot contain certain other characters such as a comma or a space. As a general rule of thumb, object names should be short and meaningful. Misleading or overly long object names will make it a pain to debug your code. Below are examples of various object name conventions. My best advice would be to pick one and stick with it. this_is_snake_case other.people.use.periods evenOthersUseCamelCase Let’s make another assignment: this_is_a_really_long_name &lt;- 2.5 To inspect the object we’ve just created, try out RStudio’s auto-complete feature: type the first few characters, press TAB, add characters until you get what you want, then press return/enter. 2.3 Functions We will use functions in most of our work with R, either pre-written or ones we write ourselves. Functions help us easily repeat instructions and carry out multiple tasks in a single step, saving us a lot of space in our code. You can call functions like this: functionName(arg1 = val1, arg2 = val2, ...) Notice that we use = instead of &lt;- within a function. Here, arg1 and arg2 are the arguments of the function. Likewise, val1 and val2 are the parameters of arg1 and arg2. Let’s try using seq() which makes regular sequences of numbers: seq(1, 10) ## [1] 1 2 3 4 5 6 7 8 9 10 The above snippet also demonstrates something about how R resolves function arguments. You can always specify name = value if you’re unsure. If you don’t, R attempts to resolve by position. In the above snippet, R assumed we wanted a sequence from = 1 that goes to = 10. As an exercise, try creating a sequence of numbers from 1 to 10 by increments of 2: If you just make an assignment, you don’t see the assigned value. To show the assigned value, just call the variable. one_to_ten &lt;- seq(1, 10) one_to_ten ## [1] 1 2 3 4 5 6 7 8 9 10 You can shorten this common action by surrounding the assignment with parentheses. (one_to_ten &lt;- seq(1, 10)) ## [1] 1 2 3 4 5 6 7 8 9 10 Not all functions have (or require) arguments: date() ## [1] &quot;Mon Aug 30 22:59:15 2021&quot; If you’ve been following along in RStudio, look at your workspace (in the upper right pane.) The workspace is where user-defined objects accumulate. You can also get a listing of these objects with commands: objects() ls() If you want to remove the object named one_to_ten, you can do this: rm(one_to_ten) To remove everything: rm(list = ls()) or click the broom icon in RStudio’s Environment pane. 2.4 Math operators Here are some basic math operations you can perform in R. Try playing around with them in the interactive window. 2.5 Conditionals Conditional statements check if a condition is true or false using logical operators (operators that return either TRUE or FALSE). For example: 20 == 10*2 ## [1] TRUE &quot;hello&quot; == &quot;goodbye&quot; ## [1] FALSE These statements return a value is of type \"logical\", which is either TRUE if the condition is satisfied, or FALSE if the condition is not satisfied. One important note is that TRUE and FALSE are objects on their own, rather than the strings “true” and “false.” Conditional statements are made with a range of logical operators. Here are some examples: Operator Plain English == is equal to != is not equal to &lt; or &gt; is less than OR is greater than &lt;= or &gt;= is less than or equal to OR is greater than or equal to is.na() is an NA value There are other logical operators, including %in%, which checks if a value is present in a vector of possible values. Try playing around with the following statements and checking their output by running the code. If you’re keen, you’ll notice in the last line that we use c() to group objects together. This data structure is called a vector. As a brief introduction, vectors combine objects of the same type. Don’t worry too much about the specifics of vectors, as we will cover it in much greater depth in the next chapter. We can also combine conditions using the logical and (&amp;) along with the logical or (|). The logical &amp; returns TRUE if and only if both conditions are true, and it returns FALSE otherwise. Let’s look at the following examples: # is (5 greater than 2) AND (6 greater than 10)? (5 &gt; 2) &amp; (6 &gt;= 10) ## [1] FALSE # is (5 greater than 2) OR (6 greater than 10)? (5 &gt; 2) | (6 &gt;= 10) ## [1] TRUE 2.5.1 If statements Conditional statements generate logical values to filter inputs. if statements use conditional statements to control flow of a program. Below is the general form of an if statement: if (the conditional statement is TRUE) { do something } Let’s look at an example: Try assigning 6 to x and predict the output. Although an if statement alone is handy, we often want to check multiple conditions. We can add more conditions and associated actions with else if statements. Suppose we want to send an automated message to our friends. Here’s how we can do it: friend &lt;- &quot;Jasmine&quot; if (friend == &quot;Jason&quot;) { msg &lt;- &quot;Hi, Jason!&quot; } else if (friend == &quot;Jasmine&quot;) { msg &lt;- &quot;How are you, Jasmine?&quot; } msg ## [1] &quot;How are you, Jasmine?&quot; We can specify what to do if none of the conditions are TRUE by using else on its own. Try modifying the code below to print “Stranger danger!” if our friend’s name isn’t “Jason” or “Jasmine.” 2.6 Working directory Any process running on your computer has a notion of its “working directory.” By default in R, a working directory is where R will look for files you ask it to load. It is also where any files you write to disk will go. You can explicitly get your working directory with the getwd() function: getwd() The working directory is also displayed at the top of the RStudio console. You can set your working directory at the command line like so: setwd(&quot;path-to-my-directory/&quot;) The setwd() function is extremely useful for times you want to read in external data, such as a .csv file. 2.6.1 Other important things Below is a collection of important miscellaneous items to consider. R scripts are usually saved with a .R or .r suffix. Comments start with one or more # symbols. Use them. RStudio helps you (de)comment selected lines with Ctrl+Shift+C (Windows and Linux) or Command+Shift+C (Mac). Clean out the workspace. You can do so by clicking the broom icon or by typing rm(list = ls()) into the console. This workflow will serve you well in the future: Create an RStudio project for an analytical project. Keep inputs there (we’ll soon talk about importing). Keep scripts there; edit them, run them in bits or as a whole from there. Keep outputs there. Avoid using your mouse for your workflow. Firstly, using the keyboard is faster. Secondly, writing code instead of clicking helps with reproducibility. That is, it will be much easier to retrospectively determine how a numerical table or PDF was actually produced. Many experienced users never save the workspace, never save .RData files (I’m one of them), and never save or consult the history. Once/if you get to that point, there are options available in RStudio to disable the loading of .RData and permanently suppress the prompt on exit to save the workspace (go to Tools &gt; Options &gt; General). "],["data-structures.html", "3 Data structures 3.1 Vectors 3.2 Lists 3.3 Data frames", " 3 Data structures In this chapter, we will learn about data structures that will greatly aid our data science workflow. 3.1 Vectors Vectors. Vectors are a sequence of values with the same type. We can create vectors using c(), which stands for “combine.” (my_nums &lt;- c(2.8, 3.2, 1.5, 3.8)) ## [1] 2.8 3.2 1.5 3.8 To access the elements inside a vector, we can do something called “slicing.” To access a single item or multiple items, use the square bracket operator []. In general [] in R means, “give me a piece of something.” For example: my_nums[4] ## [1] 3.8 my_nums[1:3] ## [1] 2.8 3.2 1.5 my_nums[c(1, 2, 3)] == my_nums[1:3] ## [1] TRUE TRUE TRUE In my_nums[1:3], the 1:3 creates a vector from 1 to 3, which is then used to subset multiple items in a vector. Here are some additional useful functions: length(my_nums) mean(my_nums) max(my_nums) min(my_nums) sum(my_nums) Given the data in the interactive block, consider the following exercises: Select “Pouria” and “Ana” from the names vector. Select all individuals who have ages greater than 20. Assume the order of names and ages correlates by index. Select all individuals whose age is not 21. Find the average age of all individuals. 3.1.1 Missing values So far we’ve worked with data with no missing values. In real life, however, we often have missing values (NA values). Unfortunately for us, R does not get along with NA values. density_ha &lt;- c(2.8, 3.2, 1.5, NA) mean(density_ha) ## [1] NA Why did we get NA? Well, it’s hard to say what a calculation including NA should be, so most calculations return NA when NA is in the data. One way to resolve this issue is to tell our function to remove the NA before executing: mean(density_ha, na.rm = TRUE) ## [1] 2.5 3.2 Lists Lists. Lists are a vector-like structure that can store elements of different typese (e.g., numbers, strings, vectors). We can create lists using the list() function. sites &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) notes &lt;- &quot;It was a good day in the field today. Warm, sunny, lots of gators.&quot; helpers &lt;- 4 field_notes &lt;- list(sites, notes, helpers) You can index lists in the following ways: field_notes[1] ## [[1]] ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; field_notes[[1]] ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; We can also give the values names and access them using the $ symbol–which is the preferred method–or via [\"variable_name\"] with subsetting. Try getting the my_sets vector from field_notes. 3.3 Data frames This is where things get really exciting! We will use these data frames extensively in the upcoming chapters, so it’s important to pay attention here. Data frames. A data frame is a table which groups equal length vectors together. You can think of data frames like a table in a spreadsheet. You can create data frames using the data.frame() function. A data frame can contain both categorical and numerical values, whereas a vector can only contain variables of the same type (i.e., all numerical, all categorical, etc.). sites &lt;- c(&quot;a&quot;, &quot;a&quot;, &quot;b&quot;, &quot;c&quot;) area_ha &lt;- c(1, 2, 3, 4) density_ha &lt;- c(2.8, 3.2, 1.5, NA) # creating the data frame surveys &lt;- data.frame(sites, density_ha, area_ha) surveys ## sites density_ha area_ha ## 1 a 2.8 1 ## 2 a 3.2 2 ## 3 b 1.5 3 ## 4 c NA 4 Here are some useful commands to investigate a data frame: str() returns the structure of a data frame. length() returns the length of a data frame. ncol() returns the number of columns of a data frame (same as length()) nrow() returns the number of rows of a data frame. str(surveys) ## &#39;data.frame&#39;: 4 obs. of 3 variables: ## $ sites : chr &quot;a&quot; &quot;a&quot; &quot;b&quot; &quot;c&quot; ## $ density_ha: num 2.8 3.2 1.5 NA ## $ area_ha : num 1 2 3 4 ncol(surveys) ## [1] 3 nrow(surveys) ## [1] 4 Subsetting data frames is extremely similar to that for vectors. This time, however, we need to consider both rows and columns. We can access a specific member like this: my_data_frame[row, column]. Try playing around with the code below :) 3.3.1 External data We can read in external data using theread.csv() function. The main argument is the location of the data, which is either a url or a path on your computer. shrub_data &lt;- read.csv(&#39;https://datacarpentry.org/semester-biology/data/shrub-dimensions-labeled.csv&#39;) 3.3.2 Factors Let’s use the str() function to get more information about our variable shrub_data. str(shrub_data) ## &#39;data.frame&#39;: 10 obs. of 4 variables: ## $ shrubID: chr &quot;a1&quot; &quot;a2&quot; &quot;b1&quot; &quot;b2&quot; ... ## $ length : num 2.2 2.1 2.7 3 3.1 2.5 1.9 1.1 3.5 2.9 ## $ width : num 1.3 2.2 1.5 4.5 3.1 2.8 1.8 0.5 2 2.7 ## $ height : num 9.6 7.6 2.2 1.5 4 3 4.5 2.3 7.5 3.2 Notice that the shrubID column has type Factor. A factor is a special data type (NOT data structure) in R for categorical data. Factors are useful for statistics, but can mess up some aspects of computation as we’ll see in future chapters. shrub_data &lt;- read.csv(&#39;https://datacarpentry.org/semester-biology/data/shrub-dimensions-labeled.csv&#39;, stringsAsFactors = FALSE) str(shrub_data) ## &#39;data.frame&#39;: 10 obs. of 4 variables: ## $ shrubID: chr &quot;a1&quot; &quot;a2&quot; &quot;b1&quot; &quot;b2&quot; ... ## $ length : num 2.2 2.1 2.7 3 3.1 2.5 1.9 1.1 3.5 2.9 ## $ width : num 1.3 2.2 1.5 4.5 3.1 2.8 1.8 0.5 2 2.7 ## $ height : num 9.6 7.6 2.2 1.5 4 3 4.5 2.3 7.5 3.2 "],["loops-and-functions.html", "4 Loops and functions 4.1 For loops 4.2 Functions", " 4 Loops and functions Loops are fundamental a programming concept as they get a lot of repetitive stuff done in very few lines of code. Paired with custom functions, we can begin to tackle complex programming problems. 4.1 For loops For loops. Here’s what the syntax of a for loop looks like: for (item in list_of_items) { do_something(item) } And here is an example: for (i in 1:3) { print(i) } ## [1] 1 ## [1] 2 ## [1] 3 In the previous example, we used the dummy variable i to take on some range of values. Notice that i can be called anything you want. Try creating a for loop that prints the square of a number plus one for numbers ranging from 2 to 6. 4.1.1 Looping over multiple files We turn our attention now to a (slightly more) useful example: how do we analyze multiple files with similar contents? In this hypothetical example, we have 5 datasets with satellite coodinates at specific points orbiting the Earth. Suppose the files are similarly named (click on the files to download them): locations-2016-01-01.txt locations-2016-01-02.txt locations-2016-01-03.txt locations-2016-01-04.txt locations-2016-01-05.txt Our goal is to determine the number of satellite coordinates per file. First, retrieve the name of each file. my_dir &lt;- &quot;data/04_intro-to-r&quot; # files are located in this location (on my computer) my_files &lt;- &quot;locations-.*.txt&quot; # file names follow this pattern data_files &lt;- list.files(path = my_dir, pattern = my_files, full.names = TRUE) Note that the asterisk in \"*.txt\" refers to “any name in this directory” whereas the \".txt\" part ensures we are only selecting .txt files. Next, determine the number of observations in each file. We will assume that each row corresponds to a single coordinate. results &lt;- vector(mode = &quot;integer&quot;, length = length(data_files)) for (i in 1:length(data_files)) { data &lt;- read.csv(data_files[i]) count &lt;- nrow(data) results[i] &lt;- count } Now, store the output in a data frame and associate the file name with the count. # initializing the data frame with empty columns results &lt;- data.frame(file_name = character(length(data_files)), count = integer(length(data_files)), stringsAsFactors = FALSE) # reading the data into the data frame for (i in 1:length(data_files)) { data &lt;- read.csv(data_files[i]) count &lt;- nrow(data) results$file_name[i] &lt;- data_files[i] results$count[i] &lt;- count } # voila! results ## file_name count ## 1 data/04_intro-to-r/locations-2016-01-01.txt 4 ## 2 data/04_intro-to-r/locations-2016-01-02.txt 8 ## 3 data/04_intro-to-r/locations-2016-01-03.txt 10 ## 4 data/04_intro-to-r/locations-2016-01-04.txt 10 ## 5 data/04_intro-to-r/locations-2016-01-05.txt 12 Nested loops (OPTIONAL) Sometimes, we need to loop over more than a single range of numbers. For example, what if we want to select all pixels on a 2x3 rectangular screen? Here, we need to cover both the “x” and “y” pixel coodinates: for (i in 1:2) { for (j in 1:3) { print(paste(&quot;i = &quot; , i, &quot;; j = &quot;, j, sep=&quot;&quot;)) } } ## [1] &quot;i = 1; j = 1&quot; ## [1] &quot;i = 1; j = 2&quot; ## [1] &quot;i = 1; j = 3&quot; ## [1] &quot;i = 2; j = 1&quot; ## [1] &quot;i = 2; j = 2&quot; ## [1] &quot;i = 2; j = 3&quot; 4.2 Functions Sometimes, we will need to create custom functions. Luckily, we can define our own functions! Functions. This is the general syntax for a function: function_name &lt;- function(arguments) { output_value &lt;- do_something(inputs) return(output_value) } Remark: every function returns a value. Recall from your grade-school math class that functions take an input and return an output. In R, however, a function may or may not take user-defined input. This brings us to an extremely important point: creating a function does NOT run it. You must call the function to run it. As an exercise, create a function called calc_vol that takes three parameters length, width, and height, and use those values to calculate the volume of the object. Then, call the function to calculate the volume of a 1x1x1 object and a 3x2x5 object. Since R treats functions like a black box, you can’t access a variable that was created in a function. You must save the output of a function (to a variable) to use it later. 4.2.1 Conditionals within functions We can use conditionals in a function for more complex tasks. As an exercise, create a function called pred_c19_cases to predict the number of COVID-19 cases in a population (note that these numbers are fictional): The function will have two parameters pop_size (population size) and vac_brand (vaccine brand). If the vaccine is Moderna, multiply pop_size by 0.941. If the vaccine is Pfizer, multiply pop_size by 0.950. If the vaccine is Astrazeneca, multiply pop_size by 0.870. Return the predicted cases by subtracting the number of healthy individuals from pop_size. Now that we’ve got the basics of R under our belts, we can jump into the delightful world of data science 😄. "],["data-exploration.html", "5 Data exploration 5.1 Get the gapminder data 5.2 Explore gapminder 5.3 Data frames with dplyr", " 5 Data exploration Whenever you have “spreadsheetey” data, your default data structure in R should be the data frame. Data frames are awesome because They neatly package related variables by maintaining a spreadsheet-like row-ordering. Data frames make it easy to filter rows and columns of interest. Most functions for inference, modelling, and graphing will happily take a data frame object. The set of packages known as the tidyverse takes data frames one step further and explicitly prioritizes the processing of data frames. Recall that data frames, unlike vectors or matrices in R, can hold different variable types. For example, data frames can simultaneously hold character data (e.g., subject ID or name), quantitative data (e.g., white blood cell count), and categorical information (e.g., treated vs. untreated). If you use data structures that only hold 1 type of data for data analysis, you might make the terrible mistake of spreading your data over multiple, unlinked objects. Why is this a mistake? Because you need to relate the row order in each object to every othere object, i.e., a nightmare. 5.1 Get the gapminder data We will work with some of the data from the Gapminder project. The Gapminder project contains the gapminder dataset, which summarises the progression of countries over time for statistics like life expectancy and GDP. If you haven’t installed gapminder or the tidyverse yet, you can do so like this: install.packages(&quot;gapminder&quot;, dependencies=T) install.packages(&quot;tidyverse&quot;, dependencies=T) Now load the two packages. library(gapminder) library(tidyverse) 5.2 Explore gapminder By loading the gapminder package, we now have access to a data frame by the same name. class(gapminder) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; Notice that the class (type of data structure) of the gapminder object is a tibble, the tidyverse’s version of R’s data frame. A tibble is also a data frame. Let’s check out the contents of gapminder: gapminder ## # A tibble: 1,704 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. ## 7 Afghanistan Asia 1982 39.9 12881816 978. ## 8 Afghanistan Asia 1987 40.8 13867957 852. ## 9 Afghanistan Asia 1992 41.7 16317921 649. ## 10 Afghanistan Asia 1997 41.8 22227415 635. ## # … with 1,694 more rows Although this seems like a lot of output, notice that tibbles provide a nice print method that shows the most important stuff and doesn’t fill up your console. Let’s make sense of the output: The first line refers to what we’re printing—a tibble with 1704 rows and 6 columns. Below each column heading, we see &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;. These refer to the variable type of that column. fct is short for “factor” (kind of like a categorical variable), int is short for “integer,” and dbl is short for “double” (a number with decimal places). If you’re only interested in a summary of your data frame, use str(), head() or tail(): str() will provide a sensible description of almost anything and, worst case, nothing bad can actually happen. When in doubt, just use str() on your recently created objects to get ideas about what to do next. head() displays the first 6 rows of your data frame by default, and tail() shows the last 6 rows. Play around with these functions in the interactive block below! Just for your reference, if you want to change a data frame into a tibble for nicer printing, use as_tibble()! as_tibble(my_data_frame) # my_data_frame is the thing we want to make a tibble Here are more ways to query basic info on a data frame: Function Description names() returns column names ncol() returns number of columns nrow() returns number of rows dim() returns # of rows by # of columns summary() returns a statistical summary of each column Try playing around with these functions in the interactive window. 5.2.1 Importing and exporting data We can export data frames to a comma-separated values (.csv) file. write.csv(gapminder, file = &quot;data/03_data-frames/gapminder.csv&quot;) Comma-separated value files are the preferred way of importing and exporting data as it contains no formatting. Other common formats include tab-separated values (.tsv) and Excel files (.xls or .xlsx). In addition to writing to a .csv file, we can also read .csv files into R. It’s as simple as read.csv()! gapminder2 &lt;- read.csv(&quot;data/03_data-frames/gapminder.csv&quot;, header = TRUE) class(gapminder2) ## [1] &quot;data.frame&quot; As you can see,read.csv() returns a data frame object by default. Notice that we specify that header = TRUE because our first row in the .csv file is a header. Also notice that we specified a file path to our .csv file. 5.2.2 Exploring variables in a data frame To specify a single variable from a data frame, use the dollar sign $. Let’s explore gapminder’s lifeExp column by providing the proper arguments to the following functions: Let’s continue to explore gapminder. Take a look at the year variable’s class: class(gapminder$year) ## [1] &quot;integer&quot; Notice that year holds integers. On the other hand, continent holds categorical information, which is called a factor in R. class(gapminder$continent) ## [1] &quot;factor&quot; Now, I want to illustrate something important: summary(gapminder$year) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1952 1966 1980 1980 1993 2007 summary(gapminder$continent) ## Africa Americas Asia Europe Oceania ## 624 300 396 360 24 Notice that the same function returned different outputs for different variable types—forgetting this observation can lead to confusion in the future, so make sure to check your data before analysis! Let’s check out a couple more useful functions and highlight important ideas in the meantime. Within a given column/variable, table() returns the number of observations, levels() returns unique values, and nlevels() returns the number of unique values. table(gapminder$continent) ## ## Africa Americas Asia Europe Oceania ## 624 300 396 360 24 levels(gapminder$continent) ## [1] &quot;Africa&quot; &quot;Americas&quot; &quot;Asia&quot; &quot;Europe&quot; &quot;Oceania&quot; nlevels(gapminder$continent) ## [1] 5 The levels of the factor continent are “Africa,” “Americas,” etc.—this is what’s usually presented to your eyeballs by R. Behind the scenes, R assigns integer values (i.e., 1, 2, 3, …) to each level. Never ever ever forget this fact. Look at the result from str(gapminder$continent) if you are skeptical: str(gapminder$continent) ## Factor w/ 5 levels &quot;Africa&quot;,&quot;Americas&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... Specifically in modelling and figure-making, factors are anticipated and accommodated by the functions and packages you will want to exploit. Note that factors do NOT contain integers. Factors are a numerical way that R uses to represent categorical data. Tl;dr, factors are categorical variables whereas levels are unique values within a factor. Data frame summary. Use data frames and the tidyverse! The tidyverse provides a special type of data frame called a “tibble” that has nice default printing behavior, among other benefits. When in doubt, str() something or print something. Understand what your variable types are. Use factors! (but with intention and care) Do basic statistical and visual sanity checking of each variable. Refer to variables by name (ex: gapminder$lifeExp) and NOT by column number. Your code will be more robust and readable. 5.3 Data frames with dplyr dplyr is a package for data manipulation. It is built to be fast, highly expressive, and open-minded about how your data is stored. It is installed as part of the the tidyverse meta-package and it is among the packages loaded via library(tidyverse). Here’s a bit of fun trivia: dplyr stands for “data frame pliers.” 5.3.1 Subsetting data If you feel the urge to store a little snippet of your data: canada &lt;- gapminder[241:252, ] Stop and ask yourself, “Do I want to create a separate subset of my original data?” If “YES,” use proper data wrangling techniques. Alternatively, only subset the data as a temporary measure while you develop your elegant code. If “NO,” then don’t subset! Copies and excerpts of your data clutter your workspace, invite mistakes, and sow general confusion. Avoid whenever possible. Reality can also lie somewhere in between. You will find the workflows presented below can help you accomplish your goals with minimal creation of temporary, intermediate objects. Recall therm() function, which removes unwanted variable(s). x &lt;- &#39;thing to not keep&#39; print(x) rm(x) # print(x) # gives an error because x is deleted 5.3.2 Filter rows with filter() filter(). filter() takes logical expressions and returns the rows for which all are TRUE. Use this function when you want to subset observations based on row values. The first argument is the name of the data frame. The subsequent arguments are the expressions that filter the dataframe. For example, let’s filter all rows from gapminder where life expectancy is less than 29 years. filter(gapminder, lifeExp &lt; 29) ## # A tibble: 2 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Rwanda Africa 1992 23.6 7290203 737. When you run this line of code, dplyr filters the data and returns a new data frame. dplyr functions never modify their inputs, so if you want to save the result, you need to use the assignment operator, &lt;-. Let’s try this out! Here we filter based on country and year: rwanda_gthan_1979 &lt;- filter(gapminder, country == &quot;Rwanda&quot;, year &gt; 1979) Compare with some base R code to accomplish the same things: gapminder[gapminder$lifeExp &lt; 29, ] # indexing is distracting subset(gapminder, country == &quot;Rwanda&quot;) # almost same as filter; quite nice actually What if you want to filter rows based on multiple values in a variable? For example, what if we want to filter all rows with either Rwanda or Afghanistan as countries? filter(gapminder, country == &quot;Rwanda&quot; | country == &quot;Afghanistan&quot;) Recall that the Boolean operator, |, means “or.” What if we want to keep more than just 2 countries? One way would be to string Boolean operators together like so: country == \"Canada\" | country == \"Rwanda\" | country == \"Afghanistan | ... This, however, is very wordy. A useful shortcut is to use x %in% y. This selects every row where x is one of the values in y: filter(gapminder, country %in% c(&quot;Rwanda&quot;, &quot;Afghanistan&quot;)) filter(gapminder, country %in% c(&quot;Canada&quot;, &quot;Rwanda&quot;, &quot;Afghanistan&quot;)) Under no circumstances should you subset your data the way I did at first: excerpt &lt;- gapminder[241:252, ] Why is this a terrible idea? It is not self-documenting. What is so special about rows 241 through 252? It is fragile. This line of code will produce different results if someone changes the row order of gapminder, e.g. sorts the data earlier in the script. filter(gapminder, country == &quot;Canada&quot;) The above function explains itself and is fairly robust. 5.3.3 Pipe operator %&gt;% Before we go any further, we should exploit the new pipe operator that the tidyverse imports from the magrittr package by Stefan Bache. Here’s what it looks like: %&gt;%. The RStudio keyboard shortcut: Ctrl + Shift + M (Windows), Cmd + Shift + M (Mac). Let’s demo then I’ll explain: gapminder %&gt;% head() ## # A tibble: 6 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. The above code is equivalent to head(gapminder). The pipe operator takes the thing on the left-hand-side and pipes it into the function call on the right-hand-side. It literally drops it in as the first argument. You can think of an argument as your input to a function. If you remember your grade school math, functions in R do exactly what you’ve learned in school – it takes inputs (arguments/parameters) and spits an output, or a return value. Never fear, you can still specify other arguments to this function! To see the first 3 rows of Gapminder, we could say head(gapminder, 3) or this: gapminder %&gt;% head(3) ## # A tibble: 3 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. You are probably not impressed yet, but the magic will happen soon. Pure, predictable, pipeable (OPTIONAL) We’ve barely scratched the surface of dplyr but I want to point out key things you may start to appreciate. dplyr’s verbs, such as filter() and select(), are what’s called pure functions. To quote from Wickham’s Advanced R Programming book: The functions that are the easiest to understand and reason about are pure functions: functions that always map the same input to the same output and have no other impact on the workspace. In other words, pure functions have no side effects: they don’t affect the state of the world in any way apart from the value they return. And finally, the data is always the very first argument of every dplyr function. 5.3.4 Select Columns with select() select(). Use select() to subset the data on variables or columns. Here’s a simple example: select(gapminder, year, lifeExp) ## # A tibble: 1,704 x 2 ## year lifeExp ## &lt;int&gt; &lt;dbl&gt; ## 1 1952 28.8 ## 2 1957 30.3 ## 3 1962 32.0 ## 4 1967 34.0 ## 5 1972 36.1 ## 6 1977 38.4 ## 7 1982 39.9 ## 8 1987 40.8 ## 9 1992 41.7 ## 10 1997 41.8 ## # … with 1,694 more rows And here’s the same operation, but written with the pipe operator and piped through head(): gapminder %&gt;% select(year, lifeExp) %&gt;% head(4) ## # A tibble: 4 x 2 ## year lifeExp ## &lt;int&gt; &lt;dbl&gt; ## 1 1952 28.8 ## 2 1957 30.3 ## 3 1962 32.0 ## 4 1967 34.0 Think: “Take gapminder, then select the variables year and lifeExp, then show the first 4 rows.” If we didn’t have the pipe operator, this is what the above function would look like: head(select(gapminder, year, lifeExp), 4) ## # A tibble: 4 x 2 ## year lifeExp ## &lt;int&gt; &lt;dbl&gt; ## 1 1952 28.8 ## 2 1957 30.3 ## 3 1962 32.0 ## 4 1967 34.0 As you can see, this is way harder to read. That’s why the pipe operator is so useful. An important note is that select does not actually filter any rows. It simply selects columns. select() used alongisde everything() is also quite handy if you want to move variables within your data frame. The everything() function selects all variables not explicitly mentioned in select(). For example, let’s move year and continent to the front of the gapminder tibble: select(gapminder, year, continent, everything()) ## # A tibble: 1,704 x 6 ## year continent country lifeExp pop gdpPercap ## &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1952 Asia Afghanistan 28.8 8425333 779. ## 2 1957 Asia Afghanistan 30.3 9240934 821. ## 3 1962 Asia Afghanistan 32.0 10267083 853. ## 4 1967 Asia Afghanistan 34.0 11537966 836. ## 5 1972 Asia Afghanistan 36.1 13079460 740. ## 6 1977 Asia Afghanistan 38.4 14880372 786. ## 7 1982 Asia Afghanistan 39.9 12881816 978. ## 8 1987 Asia Afghanistan 40.8 13867957 852. ## 9 1992 Asia Afghanistan 41.7 16317921 649. ## 10 1997 Asia Afghanistan 41.8 22227415 635. ## # … with 1,694 more rows Here’s the data for Cambodia, but only certain variables… gapminder %&gt;% filter(country == &quot;Cambodia&quot;) %&gt;% select(year, lifeExp) ## # A tibble: 12 x 2 ## year lifeExp ## &lt;int&gt; &lt;dbl&gt; ## 1 1952 39.4 ## 2 1957 41.4 ## 3 1962 43.4 ## 4 1967 45.4 ## 5 1972 40.3 ## 6 1977 31.2 ## 7 1982 51.0 ## 8 1987 53.9 ## 9 1992 55.8 ## 10 1997 56.5 ## 11 2002 56.8 ## 12 2007 59.7 … and what a typical base R call would look like: gapminder[gapminder$country == &quot;Cambodia&quot;, c(&quot;year&quot;, &quot;lifeExp&quot;)] ## # A tibble: 12 x 2 ## year lifeExp ## &lt;int&gt; &lt;dbl&gt; ## 1 1952 39.4 ## 2 1957 41.4 ## 3 1962 43.4 ## 4 1967 45.4 ## 5 1972 40.3 ## 6 1977 31.2 ## 7 1982 51.0 ## 8 1987 53.9 ## 9 1992 55.8 ## 10 1997 56.5 ## 11 2002 56.8 ## 12 2007 59.7 Additional resources and cheatsheets for dplyr Package home on CRAN Note there are several vignettes, with the introduction being the most relevant right now. Development home on GitHub. RStudio Data Wrangling cheatsheet, covering dplyr and tidyr. Remember you can get to these via Help &gt; Cheatsheets. Excellent slides on pipelines and dplyr by TJ Mahr, talk given to the Madison R Users Group. Blog post Hands-on dplyr tutorial for faster data manipulation in R by Data School, that includes a link to an R Markdown document and links to videos. Cheatsheet from R Studio for dplyr. "],["more-dplyr.html", "6 More dplyr 6.1 Review and preparation 6.2 Use mutate() to add new variables 6.3 Use arrange() to row-order data 6.4 Use rename() to rename variables 6.5 Perform tasks on subsets with group_by() 6.6 Comprehensive practice 6.7 Data wrangling summary", " 6 More dplyr In the previous chapter, we introduced three important data wrangling concepts: filter() for subsetting rows select() for subsetting columns (i.e., variables) The pipe operator %&gt;%, which feeds the left-hand side as the first argument to the expression on the right-hand side We also discussed dplyr’s role inside the tidyverse and tibbles: dplyr is a core package in the tidyverse meta-package. Since we often make incidental usage of the others, we will load dplyr and the others via library(tidyverse). The tidyverse embraces a special flavor of data frame, called a tibble. The gapminder dataset is stored as a tibble. In this chapter, we will introduce more data wrangling tools and consolidate or skills with a comprehensive practice question. 6.1 Review and preparation Let’s load the tidyverse and gapminder. library(tidyverse) library(gapminder) We’re going to make changes to the gapminder tibble. To eliminate any fear that you’re damaging the data that comes with the package, let’s create an explicit copy of gapminder for our experiments. Don’t worry if you modify the gapminder package, since your changes are temporary (i.e., you can reload the gapminder to get a fresh dataset). my_gap &lt;- gapminder Pay close attention when we evaluate statements but let the output just print to screen… ## let output print to screen, but do not store my_gap %&gt;% filter(country == &quot;Canada&quot;) ## # A tibble: 12 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Canada Americas 1952 68.8 14785584 11367. ## 2 Canada Americas 1957 70.0 17010154 12490. ## 3 Canada Americas 1962 71.3 18985849 13462. ## 4 Canada Americas 1967 72.1 20819767 16077. ## 5 Canada Americas 1972 72.9 22284500 18971. ## 6 Canada Americas 1977 74.2 23796400 22091. ## 7 Canada Americas 1982 75.8 25201900 22899. ## 8 Canada Americas 1987 76.9 26549700 26627. ## 9 Canada Americas 1992 78.0 28523502 26343. ## 10 Canada Americas 1997 78.6 30305843 28955. ## 11 Canada Americas 2002 79.8 31902268 33329. ## 12 Canada Americas 2007 80.7 33390141 36319. … versus when we assign the output to a new variable, or overwritting one that already exists. ## store the output as an R object my_precious &lt;- my_gap %&gt;% filter(country == &quot;Canada&quot;) 6.2 Use mutate() to add new variables Imagine we wanted to recover each country’s GDP. After all, the gapminder data has a variable for population and GDP per capita. Let’s multiply them together to get the GDP of the whole country. The mutate() function defines and inserts new variables into a data frame. You can refer to existing variables by name. my_gap %&gt;% mutate(gdp = pop * gdpPercap) ## # A tibble: 1,704 x 7 ## country continent year lifeExp pop gdpPercap gdp ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. 6567086330. ## 2 Afghanistan Asia 1957 30.3 9240934 821. 7585448670. ## 3 Afghanistan Asia 1962 32.0 10267083 853. 8758855797. ## 4 Afghanistan Asia 1967 34.0 11537966 836. 9648014150. ## 5 Afghanistan Asia 1972 36.1 13079460 740. 9678553274. ## 6 Afghanistan Asia 1977 38.4 14880372 786. 11697659231. ## 7 Afghanistan Asia 1982 39.9 12881816 978. 12598563401. ## 8 Afghanistan Asia 1987 40.8 13867957 852. 11820990309. ## 9 Afghanistan Asia 1992 41.7 16317921 649. 10595901589. ## 10 Afghanistan Asia 1997 41.8 22227415 635. 14121995875. ## # … with 1,694 more rows If you don’t want to add a new column to your tibble, you can use transmute(). It works just like mutate() except it only keeps the column(s) you specify. Let’s save our output in a new tibble called gap_gdp. Recall that saving the return of functions generally suppresses printing to the console. If you want to see the output, either type or print the variable. gap_gdp &lt;- my_gap %&gt;% transmute(country, gdp = pop * gdpPercap) gap_gdp # or use print(gap_gdp) ## # A tibble: 1,704 x 2 ## country gdp ## &lt;fct&gt; &lt;dbl&gt; ## 1 Afghanistan 6567086330. ## 2 Afghanistan 7585448670. ## 3 Afghanistan 8758855797. ## 4 Afghanistan 9648014150. ## 5 Afghanistan 9678553274. ## 6 Afghanistan 11697659231. ## 7 Afghanistan 12598563401. ## 8 Afghanistan 11820990309. ## 9 Afghanistan 10595901589. ## 10 Afghanistan 14121995875. ## # … with 1,694 more rows Hmmm… those GDP numbers are almost uselessly large and abstract. Consider the advice of Randall Munroe of xkcd: One thing that bothers me is large numbers presented without context… ‘If I added a zero to this number, would the sentence containing it mean something different to me?’ If the answer is ‘no,’ maybe the number has no business being in the sentence in the first place.\" Maybe it would be more meaningful to consumers of my tables and figures to stick with GDP per capita. But what if I reported GDP per capita, relative to some benchmark country. Since Canada is my home country, I’ll go with that. I need to create a new variable that is gdpPercap divided by Canadian gdpPercap, taking care that I always divide two numbers that pertain to the same year. Here is what we need to do: Filter down to the rows for Canada. Create a new temporary variable in my_gap: Extract the gdpPercap variable from the Canadian data. Replicate it once per country in the dataset, so it has the right length. Divide raw gdpPercap by this Canadian figure. Discard the temporary variable of replicated Canadian gdpPercap. ctib &lt;- my_gap %&gt;% filter(country == &quot;Canada&quot;) ## this is a semi-dangerous way to add this variable ## I&#39;d prefer to join on year, but we haven&#39;t covered joins yet my_gap &lt;- my_gap %&gt;% mutate(tmp = rep(ctib$gdpPercap, nlevels(country)), gdpPercapRel = gdpPercap / tmp, tmp = NULL) Note that, mutate() builds new variables sequentially so you can reference earlier ones (like tmp) when defining later ones (like gdpPercapRel). Also, you can get rid of a variable by setting it to NULL. How could we sanity check that this worked? The Canadian values for gdpPercapRel better all be 1! my_gap %&gt;% filter(country == &quot;Canada&quot;) %&gt;% select(country, year, gdpPercapRel) ## # A tibble: 12 x 3 ## country year gdpPercapRel ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Canada 1952 1 ## 2 Canada 1957 1 ## 3 Canada 1962 1 ## 4 Canada 1967 1 ## 5 Canada 1972 1 ## 6 Canada 1977 1 ## 7 Canada 1982 1 ## 8 Canada 1987 1 ## 9 Canada 1992 1 ## 10 Canada 1997 1 ## 11 Canada 2002 1 ## 12 Canada 2007 1 I perceive Canada to be a “high GDP” country, so I predict that the distribution of gdpPercapRel is located below 1, possibly even well below. Check your intuition! summary(my_gap$gdpPercapRel) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.007236 0.061648 0.171521 0.326659 0.446564 9.534690 The relative GDP per capita numbers are, in general, well below 1. We see that most of the countries covered by this dataset have substantially lower GDP per capita, relative to Canada, across the entire time period. Remember: Trust No One. Including (especially?) yourself. Always try to find a way to check that you’ve done what meant to. Prepare to be horrified. 6.3 Use arrange() to row-order data arrange(). The arrange() function reorders rows in a data frame. Imagine you wanted my_gap ordered by year then country, as opposed to by country then year. my_gap %&gt;% arrange(year, country) ## # A tibble: 1,704 x 7 ## country continent year lifeExp pop gdpPercap gdpPercapRel ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. 0.0686 ## 2 Albania Europe 1952 55.2 1282697 1601. 0.141 ## 3 Algeria Africa 1952 43.1 9279525 2449. 0.215 ## 4 Angola Africa 1952 30.0 4232095 3521. 0.310 ## 5 Argentina Americas 1952 62.5 17876956 5911. 0.520 ## 6 Australia Oceania 1952 69.1 8691212 10040. 0.883 ## 7 Austria Europe 1952 66.8 6927772 6137. 0.540 ## 8 Bahrain Asia 1952 50.9 120447 9867. 0.868 ## 9 Bangladesh Asia 1952 37.5 46886859 684. 0.0602 ## 10 Belgium Europe 1952 68 8730405 8343. 0.734 ## # … with 1,694 more rows Or maybe you want just the data from 2007, sorted on life expectancy? my_gap %&gt;% filter(year == 2007) %&gt;% arrange(lifeExp) ## # A tibble: 142 x 7 ## country continent year lifeExp pop gdpPercap gdpPercapRel ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Swaziland Africa 2007 39.6 1.13e6 4513. 0.124 ## 2 Mozambique Africa 2007 42.1 2.00e7 824. 0.0227 ## 3 Zambia Africa 2007 42.4 1.17e7 1271. 0.0350 ## 4 Sierra Leone Africa 2007 42.6 6.14e6 863. 0.0237 ## 5 Lesotho Africa 2007 42.6 2.01e6 1569. 0.0432 ## 6 Angola Africa 2007 42.7 1.24e7 4797. 0.132 ## 7 Zimbabwe Africa 2007 43.5 1.23e7 470. 0.0129 ## 8 Afghanistan Asia 2007 43.8 3.19e7 975. 0.0268 ## 9 Central African Repub… Africa 2007 44.7 4.37e6 706. 0.0194 ## 10 Liberia Africa 2007 45.7 3.19e6 415. 0.0114 ## # … with 132 more rows Oh, you’d like to sort on life expectancy in descending order? Then use desc(). my_gap %&gt;% filter(year == 2007) %&gt;% arrange(desc(lifeExp)) ## # A tibble: 142 x 7 ## country continent year lifeExp pop gdpPercap gdpPercapRel ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Japan Asia 2007 82.6 127467972 31656. 0.872 ## 2 Hong Kong, China Asia 2007 82.2 6980412 39725. 1.09 ## 3 Iceland Europe 2007 81.8 301931 36181. 0.996 ## 4 Switzerland Europe 2007 81.7 7554661 37506. 1.03 ## 5 Australia Oceania 2007 81.2 20434176 34435. 0.948 ## 6 Spain Europe 2007 80.9 40448191 28821. 0.794 ## 7 Sweden Europe 2007 80.9 9031088 33860. 0.932 ## 8 Israel Asia 2007 80.7 6426679 25523. 0.703 ## 9 France Europe 2007 80.7 61083916 30470. 0.839 ## 10 Canada Americas 2007 80.7 33390141 36319. 1 ## # … with 132 more rows I advise that your analyses NEVER rely on rows or variables being in a specific order. But it’s still true that human beings write the code and the interactive development process can be much nicer if you reorder the rows of your data as you go along. Also, once you are preparing tables for human eyeballs, it is imperative that you step up and take control of row order. 6.4 Use rename() to rename variables rename(). As the function name suggests, rename() will rename variables in your data frame. When I started programming, I was a camelCase person, but now I’m all about snake_case. Let’s rename some variables! my_gap %&gt;% rename(life_exp = lifeExp, gdp_percap = gdpPercap, gdp_percap_rel = gdpPercapRel) ## # A tibble: 1,704 x 7 ## country continent year life_exp pop gdp_percap gdp_percap_rel ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. 0.0686 ## 2 Afghanistan Asia 1957 30.3 9240934 821. 0.0657 ## 3 Afghanistan Asia 1962 32.0 10267083 853. 0.0634 ## 4 Afghanistan Asia 1967 34.0 11537966 836. 0.0520 ## 5 Afghanistan Asia 1972 36.1 13079460 740. 0.0390 ## 6 Afghanistan Asia 1977 38.4 14880372 786. 0.0356 ## 7 Afghanistan Asia 1982 39.9 12881816 978. 0.0427 ## 8 Afghanistan Asia 1987 40.8 13867957 852. 0.0320 ## 9 Afghanistan Asia 1992 41.7 16317921 649. 0.0246 ## 10 Afghanistan Asia 1997 41.8 22227415 635. 0.0219 ## # … with 1,694 more rows I did NOT assign the post-rename object back to my_gap because that would make the chunks in this tutorial harder to copy/paste and run out of order. In real life, I would probably assign this back to my_gap, in a data preparation script, and proceed with the new variable names. 6.4.1 Use select() to rename and reposition variables You’ve seen simple uses of select(). There are two tricks you might enjoy: select() can rename the variables you request to keep. select() can be used with everything() to hoist a variable up to the front of the tibble. my_gap %&gt;% filter(country == &quot;Burundi&quot;, year &gt; 1996) %&gt;% select(yr = year, lifeExp, gdpPercap) %&gt;% select(gdpPercap, everything()) ## # A tibble: 3 x 3 ## gdpPercap yr lifeExp ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 463. 1997 45.3 ## 2 446. 2002 47.4 ## 3 430. 2007 49.6 everything() is one of several helpers for variable selection. Read the documentation to see the rest. 6.5 Perform tasks on subsets with group_by() I have found collaborators love to ask seemingly innocuous questions like, “which country experienced the sharpest 5-year drop in life expectancy?” In fact, that is a totally natural question to ask. But if you are using a language that doesn’t know about data, it’s an incredibly annoying question to answer. dplyr offers powerful tools to solve this class of problem. group_by() adds extra structure to your dataset – grouping information – which lays the groundwork for computations within the groups. summarize() takes a dataset with \\(n\\) observations, computes requested summaries, and returns a dataset with 1 observation. Window functions take a dataset with \\(n\\) observations and return a dataset with \\(n\\) observations. You can also do very general computations on your groups with do(). Combined with the verbs you already know, these new tools allow you to solve an extremely diverse set of problems with relative ease. 6.5.1 Counting Let’s start with simple counting. How many observations do we have per continent? The n() function counts the number of observations in a particular group. my_gap %&gt;% group_by(continent) %&gt;% summarize(n = n()) ## # A tibble: 5 x 2 ## continent n ## &lt;fct&gt; &lt;int&gt; ## 1 Africa 624 ## 2 Americas 300 ## 3 Asia 396 ## 4 Europe 360 ## 5 Oceania 24 Let us pause here to think about the tidyverse. You could get these same frequencies using table() from base R. table(gapminder$continent) ## ## Africa Americas Asia Europe Oceania ## 624 300 396 360 24 str(table(gapminder$continent)) ## &#39;table&#39; int [1:5(1d)] 624 300 396 360 24 ## - attr(*, &quot;dimnames&quot;)=List of 1 ## ..$ : chr [1:5] &quot;Africa&quot; &quot;Americas&quot; &quot;Asia&quot; &quot;Europe&quot; ... But the object of class table that is returned makes downstream computation a bit fiddlier than we would like. For example, it’s too bad the continent levels come back only as names and not as a proper factor, with the original set of levels. The tally() function is a convenient function that counts rows. my_gap %&gt;% group_by(continent) %&gt;% tally() ## # A tibble: 5 x 2 ## continent n ## &lt;fct&gt; &lt;int&gt; ## 1 Africa 624 ## 2 Americas 300 ## 3 Asia 396 ## 4 Europe 360 ## 5 Oceania 24 The count() function is an even more convenient function that does both grouping and counting. my_gap %&gt;% count(continent) ## # A tibble: 5 x 2 ## continent n ## &lt;fct&gt; &lt;int&gt; ## 1 Africa 624 ## 2 Americas 300 ## 3 Asia 396 ## 4 Europe 360 ## 5 Oceania 24 What if we wanted to add the number of unique countries for each continent? You can compute multiple summaries inside summarize(). Use the n_distinct() function to count the number of distinct countries within each continent. my_gap %&gt;% group_by(continent) %&gt;% summarize(n = n(), n_countries = n_distinct(country)) ## # A tibble: 5 x 3 ## continent n n_countries ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; ## 1 Africa 624 52 ## 2 Americas 300 25 ## 3 Asia 396 33 ## 4 Europe 360 30 ## 5 Oceania 24 2 6.5.2 General summarization The functions you’ll apply within summarize() include classical statistical summaries, like mean(), median(), var(), sd(), mad(), IQR(), min(), and max(). Remember they are functions that take \\(n\\) inputs and distill them down into 1 output. Although this may be statistically ill-advised, let’s compute the average life expectancy by continent. my_gap %&gt;% group_by(continent) %&gt;% summarize(avg_lifeExp = mean(lifeExp)) ## # A tibble: 5 x 2 ## continent avg_lifeExp ## &lt;fct&gt; &lt;dbl&gt; ## 1 Africa 48.9 ## 2 Americas 64.7 ## 3 Asia 60.1 ## 4 Europe 71.9 ## 5 Oceania 74.3 summarize_at() applies the same summary function(s) to multiple variables. Let’s compute average and median life expectancy and GDP per capita by continent by year … but only for 1952 and 2007. my_gap %&gt;% filter(year %in% c(1952, 2007)) %&gt;% group_by(continent, year) %&gt;% summarize_at(vars(lifeExp, gdpPercap), funs(mean, median)) ## # A tibble: 10 x 6 ## # Groups: continent [5] ## continent year lifeExp_mean gdpPercap_mean lifeExp_median gdpPercap_median ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Africa 1952 39.1 1253. 38.8 987. ## 2 Africa 2007 54.8 3089. 52.9 1452. ## 3 Americas 1952 53.3 4079. 54.7 3048. ## 4 Americas 2007 73.6 11003. 72.9 8948. ## 5 Asia 1952 46.3 5195. 44.9 1207. ## 6 Asia 2007 70.7 12473. 72.4 4471. ## 7 Europe 1952 64.4 5661. 65.9 5142. ## 8 Europe 2007 77.6 25054. 78.6 28054. ## 9 Oceania 1952 69.3 10298. 69.3 10298. ## 10 Oceania 2007 80.7 29810. 80.7 29810. Let’s focus just on Asia. What are the minimum and maximum life expectancies seen by year? my_gap %&gt;% filter(continent == &quot;Asia&quot;) %&gt;% group_by(year) %&gt;% summarize(min_lifeExp = min(lifeExp), max_lifeExp = max(lifeExp)) ## # A tibble: 12 x 3 ## year min_lifeExp max_lifeExp ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1952 28.8 65.4 ## 2 1957 30.3 67.8 ## 3 1962 32.0 69.4 ## 4 1967 34.0 71.4 ## 5 1972 36.1 73.4 ## 6 1977 31.2 75.4 ## 7 1982 39.9 77.1 ## 8 1987 40.8 78.7 ## 9 1992 41.7 79.4 ## 10 1997 41.8 80.7 ## 11 2002 42.1 82 ## 12 2007 43.8 82.6 Of course it would be much more interesting to see which country contributed these extreme observations. Is the minimum (maximum) always coming from the same country?We will tackle this with window functions shortly. Computing with group-wise summaries (OPTIONAL) Don’t worry too much about this section if all the data wrangling is starting to become overwhelming – it’s mainly here for the curious. Let’s make a new variable that is the years of life expectancy gained (lost) relative to 1952, for each individual country. We group by country and use mutate() to make a new variable. The first() function extracts the first value from a vector. Notice that first() is operating on the vector of life expectancies within each country group. new_var &lt;- my_gap %&gt;% group_by(country) %&gt;% select(country, year, lifeExp) %&gt;% mutate(lifeExp_gain = lifeExp - first(lifeExp)) %&gt;% filter(year &lt; 1963) new_var ## # A tibble: 426 x 4 ## # Groups: country [142] ## country year lifeExp lifeExp_gain ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 1952 28.8 0 ## 2 Afghanistan 1957 30.3 1.53 ## 3 Afghanistan 1962 32.0 3.20 ## 4 Albania 1952 55.2 0 ## 5 Albania 1957 59.3 4.05 ## 6 Albania 1962 64.8 9.59 ## 7 Algeria 1952 43.1 0 ## 8 Algeria 1957 45.7 2.61 ## 9 Algeria 1962 48.3 5.23 ## 10 Angola 1952 30.0 0 ## # … with 416 more rows Within country, we take the difference between life expectancy in year \\(i\\) and life expectancy in 1952. Therefore we always see zeroes for 1952 and, for most countries, a sequence of positive and increasing numbers. Window functions (OPTIONAL) Window functions take \\(n\\) inputs and give back \\(n\\) outputs. Furthermore, the output depends on all the values. So rank() is a window function but sum() is not. Here we use window functions based on ranks and offsets. Let’s revisit the worst and best life expectancies in Asia over time, but retaining info about which country contributes these extreme values. my_gap %&gt;% filter(continent == &quot;Asia&quot;) %&gt;% select(year, country, lifeExp) %&gt;% group_by(year) %&gt;% filter(min_rank(desc(lifeExp)) &lt; 2 | min_rank(lifeExp) &lt; 2) %&gt;% arrange(year) ## # A tibble: 24 x 3 ## # Groups: year [12] ## year country lifeExp ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 1952 Afghanistan 28.8 ## 2 1952 Israel 65.4 ## 3 1957 Afghanistan 30.3 ## 4 1957 Israel 67.8 ## 5 1962 Afghanistan 32.0 ## 6 1962 Israel 69.4 ## 7 1967 Afghanistan 34.0 ## 8 1967 Japan 71.4 ## 9 1972 Afghanistan 36.1 ## 10 1972 Japan 73.4 ## # … with 14 more rows We see that (min = Afghanistan, max = Japan) is the most frequent result, but Cambodia and Israel pop up at least once each as the min or max, respectively. That table should make you impatient for our upcoming work on tidying and reshaping data! Wouldn’t it be nice to have one row per year? How did that actually work? First, I store and view a partial that leaves off the filter() statement. All of these operations should be familiar. asia &lt;- my_gap %&gt;% filter(continent == &quot;Asia&quot;) %&gt;% select(year, country, lifeExp) %&gt;% group_by(year) asia ## # A tibble: 396 x 3 ## # Groups: year [12] ## year country lifeExp ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 1952 Afghanistan 28.8 ## 2 1957 Afghanistan 30.3 ## 3 1962 Afghanistan 32.0 ## 4 1967 Afghanistan 34.0 ## 5 1972 Afghanistan 36.1 ## 6 1977 Afghanistan 38.4 ## 7 1982 Afghanistan 39.9 ## 8 1987 Afghanistan 40.8 ## 9 1992 Afghanistan 41.7 ## 10 1997 Afghanistan 41.8 ## # … with 386 more rows Now we apply a window function: min_rank(). Since asia is grouped by year, min_rank() operates within mini-datasets, each for a specific year. Applied to the variable lifeExp, min_rank() returns the rank of each country’s observed life expectancy. FYI, the min part just specifies how ties are broken. Here is an explicit peek at these within-year life expectancy ranks, in both the (default) ascending and descending order. If you specify rank(), ties will be denoted by .5. For instance: x &lt;- c(1, 2, 3, 3, 4) min_rank(x) ## [1] 1 2 3 3 5 rank(x) ## [1] 1.0 2.0 3.5 3.5 5.0 For concreteness, I use mutate() to actually create these variables, even though I dropped this in the solution above. Let’s look at a bit of that. asia %&gt;% mutate(le_rank = min_rank(lifeExp), le_desc_rank = min_rank(desc(lifeExp))) %&gt;% filter(country %in% c(&quot;Afghanistan&quot;, &quot;Japan&quot;, &quot;Thailand&quot;), year &gt; 1995) ## # A tibble: 9 x 5 ## # Groups: year [3] ## year country lifeExp le_rank le_desc_rank ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 1997 Afghanistan 41.8 1 33 ## 2 2002 Afghanistan 42.1 1 33 ## 3 2007 Afghanistan 43.8 1 33 ## 4 1997 Japan 80.7 33 1 ## 5 2002 Japan 82 33 1 ## 6 2007 Japan 82.6 33 1 ## 7 1997 Thailand 67.5 12 22 ## 8 2002 Thailand 68.6 12 22 ## 9 2007 Thailand 70.6 12 22 Afghanistan tends to present 1’s in the le_rank variable, Japan tends to present 1’s in the le_desc_rank variable and other countries, like Thailand, present less extreme ranks. You can understand the original filter() statement now: filter(min_rank(desc(lifeExp)) &lt; 2 | min_rank(lifeExp) &lt; 2) These two sets of ranks are formed on-the-fly, within year group, and filter() retains rows with rank less than 2, which means … the row with rank = 1. Since we do for ascending and descending ranks, we get both the min and the max. If we had wanted just the min OR the max, an alternative approach using top_n() would have worked. my_gap %&gt;% filter(continent == &quot;Asia&quot;) %&gt;% select(year, country, lifeExp) %&gt;% arrange(year) %&gt;% group_by(year) %&gt;% #top_n(1, wt = lifeExp) ## gets the max top_n(1, wt = desc(lifeExp)) ## gets the min ## # A tibble: 12 x 3 ## # Groups: year [12] ## year country lifeExp ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 1952 Afghanistan 28.8 ## 2 1957 Afghanistan 30.3 ## 3 1962 Afghanistan 32.0 ## 4 1967 Afghanistan 34.0 ## 5 1972 Afghanistan 36.1 ## 6 1977 Cambodia 31.2 ## 7 1982 Afghanistan 39.9 ## 8 1987 Afghanistan 40.8 ## 9 1992 Afghanistan 41.7 ## 10 1997 Afghanistan 41.8 ## 11 2002 Afghanistan 42.1 ## 12 2007 Afghanistan 43.8 Introduction to data visualization (OPTIONAL) Although we will get into more serious plotting in future chapters, I want to give you a taste of the excitement to come. Here, we will get sampling of the almighty ggplot2 package. Let’s look at a few basic examples. If you want to compare continuous data with a few categories, either a bar plot or box plot would be a good bet. Let’s look at the 1952 gapminder data. dat.1952 &lt;- my_gap %&gt;% filter(year == 1952) ggplot(data = dat.1952, aes(x=continent, y=lifeExp)) + geom_dotplot(binaxis = &quot;y&quot;, stackdir = &quot;center&quot;, dotsize = 0.5) + geom_boxplot(alpha=0.3) ## `stat_bindot()` using `bins = 30`. Pick better value with `binwidth`. Look at this figure, what would you comment on the mean and variance of the data? Have you identified any outliers? Now suppose we had no idea about what our data looks like, but we want to check the relationship between 2 continuous variables. A great place to start would be a scatter plot: ggplot(data = dat.1952, aes(x = gdpPercap, y = lifeExp)) + geom_point() The scatter plot shows an upwards relationship—we will quantify this correlation in a future chapter. To make gdpPercap look more like a straight line, we can plot it in a base 10 log scale using the function scale_x_log10(). While we’re at it, let’s also add colours to label different continents. ggplot(data = dat.1952, aes(x = gdpPercap, y = lifeExp)) + geom_point(aes(color = continent)) + scale_x_log10() We can also remove the grey background by setting the theme: ggplot(data = dat.1952, aes(x = gdpPercap, y = lifeExp)) + geom_point(aes(color = continent)) + scale_x_log10() + theme_classic() Don’t worry too much about figures right now. We will cover data visualization in much more depth in future lessons. 6.6 Comprehensive practice So let’s answer a “simple” question: which country experienced the sharpest 5-year drop in life expectancy (le)? Recall that this excerpt of the gapminder data only has data every five years, e.g. for 1952, 1957, etc. So this really means looking at life expectancy changes between adjacent timepoints. At this point, the question is just too easy to answer, so find life expectancy by continent while we’re at it. my_gap %&gt;% select(country, year, continent, lifeExp) %&gt;% group_by(continent, country) %&gt;% # within country, take (lifeExp in year i) - (lifeExp in year i - 1) # positive means lifeExp went up, negative means it went down mutate(le_delta = lifeExp - lag(lifeExp)) %&gt;% # within country, retain the worst lifeExp change = smallest or most negative summarize(worst_le_delta = min(le_delta, na.rm = TRUE)) %&gt;% # within continent, retain the row with the lowest worst_le_delta top_n(-1, wt = worst_le_delta) %&gt;% arrange(worst_le_delta) ## `summarise()` has grouped output by &#39;continent&#39;. You can override using the `.groups` argument. ## # A tibble: 5 x 3 ## # Groups: continent [5] ## continent country worst_le_delta ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 Africa Rwanda -20.4 ## 2 Asia Cambodia -9.10 ## 3 Americas El Salvador -1.51 ## 4 Europe Montenegro -1.46 ## 5 Oceania Australia 0.170 Now this data is interesting. Take a look at the life expectancy in Rwanda in 1987 and in 1992. gapminder %&gt;% select(country, year, lifeExp) %&gt;% filter(year == 1987 | year == 1992, country == &#39;Rwanda&#39;) ## # A tibble: 2 x 3 ## country year lifeExp ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Rwanda 1987 44.0 ## 2 Rwanda 1992 23.6 Ponder the real-life implications of this output for a while. What you’re seeing here is genocide in dry statistics on average life expectancy. 6.7 Data wrangling summary Wow, we covered a lot of data wrangling! Don’t wory if you don’t understand everything the first time around. Programming takes practice, and practice makes perfect. Here are some general remarks: Break your code into pieces starting at the top, and inspect the intermediate results. That’s certainly how I was able to write such a thing. The commands presented in this lab do not leap fully formed out of anyone’s forehead—they are built up gradually, with lots of errors and refinements along the way. If your statements are difficult to read, by all means break it into pieces and make some intermediate objects. Your code should be easy to read and write when you’re done. The functions presented here should cover most of your basic data wrangling needs. If you ever need to do something more complicated, search it up! Although I have programmed for many years, I still need to do a quick Google search for documentation and StackOverflow solutions. "],["introduction-to-statistics.html", "7 Introduction to statistics 7.1 Inferential statistics 7.2 Measures of central tendency 7.3 Measures of spread", " 7 Introduction to statistics In this chapter, we will learn about how to use statistics to analyze data. 7.1 Inferential statistics Inferential statistics is all about using a sample to make inferences about the population from which the sample was drawn. For example, we might want to check if the efficacy of the Pfizer vaccine is better than that of the Moderna vaccine for preventing COVID-19 (spoiler alert: it’s not!). Figure 7.1: A visualization of inferential statistics. Sampling bias and sampling error In inferential statistics, there are two main types of errors: sampling bias and sampling error. Both describe how there are differences between the sample and the population. Sampling bias: your sample may not be representative of the population. Sampling error: the difference between a sample and population value (e.g., you measure the heights of a sample to be 160 cm when the population height is in fact 155 cm). 7.2 Measures of central tendency Measures of central tendency. The three measures of central tendency are your mean, median, and mode. Mean: the arithmetic average. Median: a value that splits ordered data into two equal halves. Mode: the most frequent value in a dataset. Figure 7.2: A visualization of measures of central tendency. Figure retrieved from this Medium Digest article. 7.3 Measures of spread The main measures of spread are range, interquartile range, standard deviation, and variance. As you may already know, the range is the difference between the largest and smallest values in a dataset. Interquartile range. the difference between the third and first quartiles in a dataset (IQR = Q3 - Q1). IQR also represents the middle 50% of your dataset. Figure 7.3: How to compute interquartile range. Figure retrieved from Boston University statistics notes. We often visualize our data with a boxplot, which relies on the concept of quartiles and IQR. Figure 7.4: A boxplot to represent data. Figure adapted from Naysan Saran’s article. Exercise Given the following numbers, compute the interquartile range: 45, 47, 52, 52, 53, 55, 56, 58, 62, 80. Figure 7.5: A boxplot to represent data. Figure adapted from BYJU’s article. Standard deviation and variance. Both standard deviation (SD) and variance (Var) measure the spread of individual data points from the mean. A bigger SD or Var means a bigger spread. A smaller SD or Var means a smaller spread. The mathematical definition of population SD (\\(\\sigma\\)) and sample SD (\\(s\\)) is: \\[\\begin{align} \\sigma &amp;= \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\mu)^2}{n}} \\\\ s &amp;= \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n-1}} \\end{align}\\] The mathematical definition of population variance (\\(\\sigma^2\\)) and sample variance (\\(s^2\\)) is: \\[\\begin{align} \\sigma^2 &amp;= \\frac{\\sum_{i=1}^n (x_i - \\mu)^2}{n} \\\\ s^2 &amp;= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n-1} \\end{align}\\] Remarks: \\(x_i\\): individual data points. \\(\\mu\\): population mean. \\(\\bar{x}\\): sample mean. \\(n\\): number of datum in your dataset. We divide by \\(n-1\\) for sample standard deviation because to account for the greater variation of sample data as opposed to population data. Exercise Given the following 7 numbers, compute the standard deviation: 9, 4, 5, 4, 12, 7, 8. \\[\\begin{align*} \\mu &amp;= \\frac{9 + 4 + 5 + 4 + 12 + 7 + 8}{7} = 7 \\\\ \\sigma &amp;= \\sqrt{\\frac{\\sum (x_i - \\mu)^2}{n}} \\\\ &amp;= \\sqrt{ \\frac{ (9-7)^2 + (4-7)^2 + \\cdots }{7} } \\\\ &amp;\\approx 2.73 \\end{align*}\\] The final measure of spread we will cover is something called standard error. Standard error. Standard error (SE or SEM) measures the amount of spread of sample means from their true population mean. \\[\\begin{equation} SE = \\frac{\\sigma}{\\sqrt{n}} \\end{equation}\\] We will use SE extensively when we cover statistical tests. Here is an excellent article on the different use cases between SD and SEM. "],["central-limit-theorem.html", "8 Central limit theorem 8.1 Motivation 8.2 CLT for means 8.3 CLT for proportions", " 8 Central limit theorem In this chapter, we will investigate the intuition behind the Central Limit Theorem (CLT). In short, the CLT states that if you have a bunch of samples and plotted the distribution of their means (not individual observations!), the distribution would look normal regardless of the population distribution. This theorem is incredibly powerful because it lets us to make inferences using statistical tests given a sufficient sample size. 8.1 Motivation Imagine that we want to investigate whether Canadians recover faster than Americans from the common cold. How do we determine the probability that Canadians indeed recover faster than Americans? The CLT helps us answer these questions by telling us that a “large enough” sampling distribution will follow a normal distribution. We can then use the properties of the normal distribution to compute the exact probability given our data. 8.2 CLT for means Before formally exploring the CLT, play around with the CLT app! In short, the app takes samples from the “Population Distribution” and plots the mean of those samples in the “Sampling Distribution” histogram. 8.2.1 CLT Part 1 CLT Part 1. If our population is normal, our sampling distribution is also normal. As always, let’s load the tidyverse. library(tidyverse) Many observed quantities follow normal distribution. Imagine that we have a population following normal distribution that has a mean of 10 and standard deviation of 2. If we draw samples from it, are we able to estimate its mean? Note that rnorm() generates a vector of random numbers from a normal distribution. (samp30 &lt;- rnorm(n=30, mean=10, sd=2)) ## [1] 11.206082 11.343455 9.172219 10.391625 6.981954 11.955095 10.559952 ## [8] 9.674684 8.708230 9.987635 10.646180 11.334216 9.851867 10.321440 ## [15] 9.700381 13.144596 7.329367 10.722894 9.634687 14.217537 12.226628 ## [22] 10.262999 8.908421 9.393388 9.313759 8.116165 5.496599 10.273037 ## [29] 13.960707 8.650610 mean(samp30) ## [1] 10.11621 Although the mean is not 10, it is pretty close. This should make sense; when we draw a sample to estimate the mean, we may get very close to the desired “true mean,” but we also expect some error. What if I repeat the estimation 1000 times with 30 samples? # initializing our means vector means10 &lt;- as.vector(NA) # taking 1000 samples for (i in 1:1000) { samp10 &lt;- rnorm(30, mean=10, sd=2) # taking a sample with n=30 means10[i] &lt;- mean(samp10) # recording sample mean } # plotting distribution of sample means ggplot(as_tibble(means10), aes(value)) + geom_density(fill=&quot;#C5EBCB&quot;) + theme_classic() What does this figure mean? Of the 1000 sample means we computed, most were very close to 10. The probability to overestimate and underestimate decreases as the estimation deviates from 10, our “true mean.” Further, the distribution of the sample means appears to follow a normal distribution. 8.2.2 CLT Part 2 CLT Part 2. Given a sufficient sample size, the sampling distribution of the means will look normal regardless of the original population distribution. Notice that the CLT part 2 is a generalized version of the CLT part 1. This time, let’s try sampling from a population that is uniformly distributed. For example, let’s create a population with 10,000 completely random numbers between 0 and 20: uniform_popn &lt;- runif(n=10000, min=0, max=20) # our population distribution ggplot(as_tibble(uniform_popn), aes(value)) + geom_density(fill=&quot;#C5EBCB&quot;) + theme_classic() Now, we are going to repeatedly sample from uniform_popn and use the mean of that particular sample as our entry. means10 &lt;- as.vector(NA) means100 &lt;- as.vector(NA) means1000 &lt;- as.vector(NA) # taking 1000 samples for (i in 1:1000) { # each iteration, draw 10, 100, and 1000 samples from uniform_popn samp10 &lt;- sample(uniform_popn, size = 10, replace = TRUE) samp100 &lt;- sample(uniform_popn, size = 100, replace = TRUE) samp1000 &lt;- sample(uniform_popn, size = 1000, replace = TRUE) # getting means of each sample means10[i] &lt;- mean(samp10) means100[i] &lt;- mean(samp100) means1000[i] &lt;- mean(samp1000) } df &lt;- rbind( data.frame(means = means10, sample_size = &quot;10&quot;), data.frame(means = means100, sample_size = &quot;100&quot;), data.frame(means = means1000, sample_size = &quot;1000&quot;) ) ggplot(df, aes(x = means, fill = sample_size)) + geom_density(alpha = 0.3) + labs(x = &quot;Sample means&quot;, y = &quot;Density&quot;, fill = &quot;Sample size&quot;) + theme_classic() Notice two things: With a larger sample size, your estimation for the mean will have a smaller variance. Even though our original population was uniform (i.e., NOT normal), our sampling distribution looks normal. In fact, any population distribution will look normal given enough sample means. As you can see, the bigger your sample size, the less variability there is, and the more the distribution looks like a normal distribution. More precisely, the bigger your sample size, the distribution of the sample means will be normally distributed, even if the population is not normally distributed. A good rule of “sufficiently large sample size” for means is n ≥ 30. Pretty cool, right? 8.3 CLT for proportions The CLT also holds for proportions. Imagine we know the probability of not having the common cold is 0.9 and having the commmon cold is 0.1. Let’s record not having the common cold as 1 and having the common cold as 0. If I have a large enough sample size, we should expect the proportion to approach 0.9. Is this the case? means100 &lt;- as.vector(NA) means1000 &lt;- as.vector(NA) means10000 &lt;- as.vector(NA) for (i in 1:10000) { sample100 &lt;- sample(c(1, 0), prob = c(0.9, 0.1), replace = TRUE, size = 100) means100[i] &lt;- mean(sample100) sample1000 &lt;- sample(c(1, 0), prob = c(0.9, 0.1), replace = TRUE, size = 1000) means1000[i] &lt;- mean(sample1000) sample10000 &lt;- sample(c(1, 0), prob = c(0.9, 0.1), replace = TRUE, size = 10000) means10000[i] &lt;- mean(sample10000) } df &lt;- rbind( data.frame(means = means100, sample_size = &quot;100&quot;), data.frame(means = means1000, sample_size = &quot;1000&quot;), data.frame(means = means10000, sample_size = &quot;10000&quot;) ) ggplot(df, aes(x = means, fill = sample_size)) + geom_density(alpha = 0.3) + labs(x = &quot;Sample means&quot;, y = &quot;Density&quot;, fill = &quot;Sample size&quot;) + theme_classic() Can you explain the pattern that we observe with 100 samples? How about the height and width of other curves? What conclusions can we draw? This example shows the power of the CLT—it allows us to predict a sampling distribution regardless of the original population. "],["hypothesis-testing-and-p-values.html", "9 Hypothesis testing and p-values 9.1 Hypothesis testing 9.2 P-values 9.3 Significance thresholds", " 9 Hypothesis testing and p-values Before diving into statistical tests, let’s take a moment to appreciate the bigger picture. Figure 9.1: A pipeline for data-driven decisions. 9.1 Hypothesis testing We often want to test our hypotheses in statistics. For example, we might create an experiment to test if a vaccine is effective at preventing COVID-19. If your results are not consistent, no one would want to use your vaccine. Here are some examples of hypotheses: If (I increase the concentration of this drug) then (the viral particles will decrease tenfold). If (I counsel patients instead of medication) then (their overall mental health score will increase by 5 points). If (independent variable) then (dependent variable). In statistics, we have two types of hypotheses: the null hypothesis and the alternative hypothesis. Null hypothesis (H\\(_0\\)). The hypothesis that there is no significant difference between specified populations (or expected value). That is, any observed difference is due to sampling or experimental error. Alternative hypothesis (H\\(_a\\)). The hypothesis that the difference observed between expected and observed values are not due to chance alone. Let’s work through an example: A researcher thinks that if COVID-19 patients get a vaccine early after diagnosis, their recovery period will be shorter. Average recovery times for COVID patients is 2 weeks for mild cases. Thus, our hypotheses are: H\\(_0: \\mu = 2\\) H\\(_a: \\mu &lt; 2\\) If the observed average recovery times is 1 week for patients who receive a vaccine for 100 patients, then we will likely reject H\\(_0\\). Otherwise, we do not reject H\\(_0\\). We NEVER accept H\\(_0\\). However, this example raises the question of when we should reject H\\(_0\\). This is where p-values come in handy. 9.2 P-values P-values (informal definition). A p-value, or probability value, is the probability of the data being as extreme as it is. We use p-values to determine if our results are statistically significant. Since p-valuese are probabilities, p must be between 0 and 1 inclusive. P-value (formal definition). The p-value is the probability of observing a sample as extreme or more extreme than the one observed, given that the null hypothesis is true. Take a the following sleep experiment as an example. In this experiment, people try to memorize as many words as possible. Then, they are split into two groups that will either sleep 4 hours or 8 hours that night. The next day, the people try to recall as many words as possible. The following figure shows the results: Figure 9.2: Figure retrieved from Prob(a)bilistic World. The resarcher wants to determine if there are differences between the words recalled between the two groups. What are the hypotheses that we are testing in this experiment? Solution Let \\(\\mu_1\\) and \\(\\mu_2\\) be the average words the 4 hour of sleep group and 8 hour of sleep group receives respectively. Then, H\\(_0: \\mu_1 - \\mu_2 = 0\\) H\\(_a: \\mu_1 - \\mu_2 \\neq 0\\) Is this performance difference significant? If p = 0.01, it is likely significant. If p = 0.85, the difference is likely not significant. To compute the p-value, we will need to learn some statistical tests, which we will learn in the next chapter. What did w learn from this example? A high p-value indicates a higher probability that your observations are likely to be observed. A low p-value indicates that your observations are NOT likely to be observed. Check out this article if you want to read more about p-values. 9.3 Significance thresholds Question: What is a “high” and “low” p-value? That is, how do we determine whether something is statistically significant? Answer: You get to choose when a p-value is “low enough!” Significance thresholds. Suppose we set a significance threshold, \\(\\alpha\\). If p ≤ \\(\\alpha\\), then we can reject the null hypothesis. Otherwise, we do not reject the null hypothesis. Further, if p ≤ \\(\\alpha\\), our results are considered “statistically significant.” Remarks: A high p-value does NOT mean your null hypothesis is true. Rather, it suggests the probability of observing your measurement is high. To beat a dead horse, let’s reiterate the following: we NEVER accept the null hypothesis. A confidence level is defined as $1 - \\(\\alpha\\), which tells us how uncertain we are about the true mean or median in a population. For example, an election article “38% of likely U.S. Voters now say their health insurance coverage has changed because of Obamacare.” If you scroll down to the bottom of the article, you’ll see this line: “The margin of sampling error is +/- 3 percentage points with a 95% level of confidence.” At timese, determining significance feels arbitrary. In a sense, it is! As scientists, we have to compare our p-value against a significance threshold that we set. For example, if our p-value is 0.08: If \\(\\alpha\\) = 0.05, the result is not significant. If \\(\\alpha\\) = 0.10, the result is significant. If \\(\\alpha\\) = 0.08, the result is significant. In academia, a common significance threshold is \\(\\alpha = 0.05\\). Please keep in mind that there is nothing magical about having \\(p &lt; 0.05\\). All it means is that there is a low probability that our observed data is less than our null hypothesis. Here is a meme I’ve curated for you to reinforce this concept: Figure 9.3: Figure retrieved from imgflip. "],["basic-statistical-tests.html", "10 Basic statistical tests 10.1 Student’s t-test 10.2 Chi-squared test 10.3 Additional resources", " 10 Basic statistical tests R contains extremely powerful tools for data science. These tools are either built-in or available from packages. Thoughout this section we hope to demonstrate best practices organizing, analyzing, and visualizing data in R. We will again work with the gapminder dataset. Let’s load the usual packages. library(gapminder) library(tidyverse) When you get a new dataset, your first instinct as a good data scientist should be to explore it. gapminder # also recall str(), head(), names() ## # A tibble: 1,704 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. ## 7 Afghanistan Asia 1982 39.9 12881816 978. ## 8 Afghanistan Asia 1987 40.8 13867957 852. ## 9 Afghanistan Asia 1992 41.7 16317921 649. ## 10 Afghanistan Asia 1997 41.8 22227415 635. ## # … with 1,694 more rows Note the variables country and continent are considered factors (fct), which is a catagorical data. We can use levels() to observe the catagories within a factor column. levels(gapminder$continent) ## [1] &quot;Africa&quot; &quot;Americas&quot; &quot;Asia&quot; &quot;Europe&quot; &quot;Oceania&quot; 10.1 Student’s t-test Student’s t-test. A t-test is used to determine if there is a significant difference between the means of two groups. We use t-tests to compare means between two samples: Given two sample means ̄y0, ̄y1: Null hypothesis (H0): sample means are equal OR the difference between sample means is equal to some value d0 (i.e., ̄y0 - ̄y1 = d0). Alternative hypothesis (Ha): sample means are not equal OR the difference between sample means is not equal to some value d0. There are four main “flavours” to the t-test (one-sample, equal variance, unequal variance, pooled). Let’s start by asking the mean life expectancy of the continents in 1952. First, we’ll summarize our data. gapminder %&gt;% filter(year == 1952) %&gt;% group_by(continent) %&gt;% summarise(mean(lifeExp)) ## # A tibble: 5 x 2 ## continent `mean(lifeExp)` ## &lt;fct&gt; &lt;dbl&gt; ## 1 Africa 39.1 ## 2 Americas 53.3 ## 3 Asia 46.3 ## 4 Europe 64.4 ## 5 Oceania 69.3 The life expectancy of Europe is about 64.4 years. It seems close to 65, the standard age often associated with retirement in Canada (and when full pension benefits become available!). Is this life expectancy significantly different from 65 years? 10.1.1 Example 1: One sample t-test To answer this question, we can use a one sample t-test. We will test the sample (life expectancy measured in Europe in 1952) to see if there is a significant difference between the life expectancy in Europe (64.4 years) and 65 years. Can you set up the appropriate hypotheses to test? Solution Let \\(\\mu\\) be the average life expectancy of people in Europe in 1952. Then, H\\(_0: \\mu = 65\\) H\\(_a: \\mu \\neq 65\\) Euro.life.1952 &lt;- gapminder %&gt;% filter(continent == &#39;Europe&#39;, year == 1952) %&gt;% select(lifeExp) t.test(Euro.life.1952, mu = 65, alternative = &quot;two.sided&quot;) ## ## One Sample t-test ## ## data: Euro.life.1952 ## t = -0.50931, df = 29, p-value = 0.6144 ## alternative hypothesis: true mean is not equal to 65 ## 95 percent confidence interval: ## 62.03323 66.78377 ## sample estimates: ## mean of x ## 64.4085 Notice that p-value is 0.6144. Usually, we choose the significance threshold (\\(\\alpha\\)) to be 0.05. Since p &lt; \\(\\alpha\\), we conclude that the life expectancy of Europeans in 1952 doesn’t give us evidence indicating a difference in life expectancy from 65. We can also plot this: ggplot() + geom_histogram(aes(Euro.life.1952$lifeExp), bins=10) + geom_vline(xintercept = 65, col=&quot;red&quot;) + labs(x = &quot;European life expectancy in 1952 (years)&quot;, y = &quot;Count&quot;) Note: This is NOT a figure you would include in an academic paper as the quality is quite low. We’re visualizing this just so we have a better idea of what’s going on with the data. Wilcoxon signed-rank test The non-parametric test equivalent to one sample t-test is Wilcoxon signed-rank test. wilcox.test(Euro.life.1952$lifeExp, mu=65) ## ## Wilcoxon signed rank exact test ## ## data: Euro.life.1952$lifeExp ## V = 238, p-value = 0.9193 ## alternative hypothesis: true location is not equal to 65 The non-parametric test gave us the same conclusion. CAUTION: Although we obtained the same results with both the parametric t-test and non-parametric signed-rank test, their use cases are VERY different. We prefer to use parametric tests because they give us more statistical power. Only use non-parametric tests with sample sizes less than 30 and if the data is not normally distributed. 10.1.2 Example 2: Two sample t-test Does Asia and Africa differ in life expectancy in 1952? To compare two groups of data, we need a two sample t-test. Can you set up the hypotheses? Solution Let \\(\\mu_1\\) be the average life expectancy of people in Asia in 1952. Let \\(\\mu_2\\) be the average life expectancy of people in Africa in 1952. Then, H\\(_0: \\mu_1 - \\mu_2 = 0\\) H\\(_a: \\mu_1 - \\mu_2 \\neq 65\\) As.Af &lt;- gapminder %&gt;% filter(year == 1952) %&gt;% filter(continent %in% c(&quot;Africa&quot;, &quot;Asia&quot;)) As.Af ## # A tibble: 85 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Algeria Africa 1952 43.1 9279525 2449. ## 3 Angola Africa 1952 30.0 4232095 3521. ## 4 Bahrain Asia 1952 50.9 120447 9867. ## 5 Bangladesh Asia 1952 37.5 46886859 684. ## 6 Benin Africa 1952 38.2 1738315 1063. ## 7 Botswana Africa 1952 47.6 442308 851. ## 8 Burkina Faso Africa 1952 32.0 4469979 543. ## 9 Burundi Africa 1952 39.0 2445618 339. ## 10 Cambodia Asia 1952 39.4 4693836 368. ## # … with 75 more rows We can plot this: ggplot(As.Af, aes(continent, lifeExp)) + geom_dotplot(binaxis = &#39;y&#39;, stackdir = &#39;center&#39;, dotsize=0.65) + geom_boxplot(alpha=0.3) + labs(x=&#39;Continent&#39;, y=&#39;Life expectancy (yrs)&#39;) ## `stat_bindot()` using `bins = 30`. Pick better value with `binwidth`. Here we want to run a two sample t-test. Before we do that, we’ll need to check if the 2 samples have the same variance. Recall that different t-tests assume different variances: If you assume equal variance, you would use Student’s t-test. If variances are unequal, use Welch’s t-test. A good rule of thumb is if the larger standard deviation (SD) divded by the smaller SD is less than 2 (SD(larger)/SD(smaller) &lt; 2), then you can assume equal variance. Alternatively, you can test for equal variances: library(car) # for Levene&#39;s test leveneTest(y = As.Af$lifeExp, group = As.Af$continent) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 1 12.514 0.0006644 *** ## 83 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Since \\(p = 0.0006644 &lt; 0.05\\), the two samples have significantly different variances. Indeed, the width of the boxplots in the figure above suggested this difference. Because we have different variances, we need to use Welch’s t-test. By default, t.test() assumes unequal variance. If this wasn’t the case, we would add an additional argument called var.equal = FALSE to t.test(). t.test(lifeExp ~ continent, As.Af, alternative = &quot;two.sided&quot;) ## ## Welch Two Sample t-test ## ## data: lifeExp by continent ## t = -4.0599, df = 44.637, p-value = 0.0001952 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -10.741084 -3.616704 ## sample estimates: ## mean in group Africa mean in group Asia ## 39.13550 46.31439 Mann-Whitney U test The non-parametric test equivalent to the two sample t-test in this case would be the Mann-Whitney U test. wilcox.test(lifeExp ~ continent, As.Af) ## ## Wilcoxon rank sum test with continuity correction ## ## data: lifeExp by continent ## W = 443, p-value = 0.0001857 ## alternative hypothesis: true location shift is not equal to 0 10.1.3 Example 3: Paired t-test Next, let’s take a look at life expectancy in 2007: gapminder %&gt;% filter(year == 2007) %&gt;% group_by(continent) %&gt;% summarise(meanlife = mean(lifeExp)) ## # A tibble: 5 x 2 ## continent meanlife ## &lt;fct&gt; &lt;dbl&gt; ## 1 Africa 54.8 ## 2 Americas 73.6 ## 3 Asia 70.7 ## 4 Europe 77.6 ## 5 Oceania 80.7 Has the life expectancy in Africa changed to that in 1952? We can answer this question with a two sample t-test. This time, we would like to match the countries. First, let’s generate a long data frame. Africa &lt;- gapminder %&gt;% filter(continent==&quot;Africa&quot;) %&gt;% select(country, year, lifeExp) head(Africa) ## # A tibble: 6 x 3 ## country year lifeExp ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Algeria 1952 43.1 ## 2 Algeria 1957 45.7 ## 3 Algeria 1962 48.3 ## 4 Algeria 1967 51.4 ## 5 Algeria 1972 54.5 ## 6 Algeria 1977 58.0 Second, let’s look at how the life expectancy changed over the years. p &lt;- ggplot(data = Africa, aes(x = year, y = lifeExp)) + geom_point(aes(color = country)) + geom_line(aes(group = country, color=country)) show(p) The change in life expectancy between 1952 and 2007 look interesting. Most of the countries have improved, while a few have decreased life expectancy. Before testing this observation, let’s reorganize our data into a nice (wide) shape. Africa.wide &lt;- spread(Africa, year, lifeExp) Africa.wide ## # A tibble: 52 x 13 ## country `1952` `1957` `1962` `1967` `1972` `1977` `1982` `1987` `1992` `1997` ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Algeria 43.1 45.7 48.3 51.4 54.5 58.0 61.4 65.8 67.7 69.2 ## 2 Angola 30.0 32.0 34 36.0 37.9 39.5 39.9 39.9 40.6 41.0 ## 3 Benin 38.2 40.4 42.6 44.9 47.0 49.2 50.9 52.3 53.9 54.8 ## 4 Botswa… 47.6 49.6 51.5 53.3 56.0 59.3 61.5 63.6 62.7 52.6 ## 5 Burkin… 32.0 34.9 37.8 40.7 43.6 46.1 48.1 49.6 50.3 50.3 ## 6 Burundi 39.0 40.5 42.0 43.5 44.1 45.9 47.5 48.2 44.7 45.3 ## 7 Camero… 38.5 40.4 42.6 44.8 47.0 49.4 53.0 55.0 54.3 52.2 ## 8 Centra… 35.5 37.5 39.5 41.5 43.5 46.8 48.3 50.5 49.4 46.1 ## 9 Chad 38.1 39.9 41.7 43.6 45.6 47.4 49.5 51.1 51.7 51.6 ## 10 Comoros 40.7 42.5 44.5 46.5 48.9 50.9 52.9 54.9 57.9 60.7 ## # … with 42 more rows, and 2 more variables: 2002 &lt;dbl&gt;, 2007 &lt;dbl&gt; The wide data frame is to align the data from the same country to the same row, so that they have the same index when we call different columns. We can now run a paired t-test. Think about what we are testing in the code below. t.test(Africa.wide$&#39;2007&#39;, Africa.wide$&#39;1952&#39;, alternative=&quot;two.sided&quot;, paired=T) ## ## Paired t-test ## ## data: Africa.wide$&quot;2007&quot; and Africa.wide$&quot;1952&quot; ## t = 13.042, df = 51, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 13.25841 18.08267 ## sample estimates: ## mean of the differences ## 15.67054 Note in this process we didn’t check the variance. Is this a problem? Why or why not? Recall that the paired t-test is actually a one sample t-test on paired differences. Similarly, we could again call wilcox.test to run the paired version. wilcox.test(Africa.wide$&#39;2007&#39;, Africa.wide$&#39;1952&#39;, paired=T) ## ## Wilcoxon signed rank test with continuity correction ## ## data: Africa.wide$&quot;2007&quot; and Africa.wide$&quot;1952&quot; ## V = 1369, p-value = 6.087e-10 ## alternative hypothesis: true location shift is not equal to 0 One-tailed and two-tailed tests Statistical tests can be either 1-tailed or 2-tailed depending on your question. 1-tailed tests have 2 possibilities (having a right tail or a left tail) whereas 2-tailed tests have 1 possibility (both right and left tail). One-tailed vs Two-tailed Tests. One-tailed tests: If you know that life expectancy increases over time, then you would use a 1-tailed t-test to test if life expectancy is greater in 2005 as compared to 1996. Two-tailed tests: Use this if you don’t know how means should look. For example, is the life expectancy in South Korea equal to the life expectancy in Canada? If in doubt, use a two-tailed test. This is because significance in a two-tailed test is “more significant” than that of the one-tailed test. 10.1.4 T-test Assumptions Since the t-test relies on a normal-like distribution, it has some assumptions. 1. Independence assumption: the data values should be independent. (a) Satisfied if the data are from randomly sampled data. 2. Normal population assumption: the data follows a normal distribution. (a) If n &lt; 30, use a histogram to check that the data is unimodal and symmetric. (b) If n ≥ 30, this assumption is fulfilled by the CLT. 3. 10% condition: when a sample is drawn without replacement, the sample should be no more than 10% of the population. • This is usually assumed to be true. If any of these conditions are not fulfilled, consider using a non-parametric test equivalent. Example Questions Question 1: How long can car crash patients jog before and after physiotherapy? Question 2: Is the mean life expectancy in Argentina different before and after 1980? Here is the equation to get the test statistic: (don’t worry too much about this) 10.2 Chi-squared test Chi-squared test. A chi-squared test determines if there is a significant difference between expected and observed data. 10.2.1 \\(\\chi^2\\) test for goodness-of-fit Let’s analyze the data in Mendel’s original paper: Mendel, Gregor. 1866. Versuche über Plflanzenhybriden. Verhandlungen des naturforschenden Vereines in Brünn, Bd. IV für das Jahr 1865, Abhandlungen, 3–47. Crash course on Mendelian genetics Let’s (briefly) learn about Mendelian genetics regarding dominant and recessive alleles! Recall that crossing two heterozygotes (Aa x Aa) produces offspring with dominant and recessive phenotypes with an expected ratio of 3:1. \\[\\begin{array}{c|cc} &amp; \\mathbf{A} &amp; \\mathbf{a} \\\\ \\hline \\mathbf{A} &amp; AA &amp; Aa \\\\ \\mathbf{a} &amp; Aa &amp; aa \\end{array}\\] Also recall that a dihybrid cross (AaBb x AaBb) produces offspring of 4 phenotypes with an expected ratio of 9:3:3:1. \\[\\begin{array}{c|cccc} &amp; \\mathbf{AB} &amp; \\mathbf{Ab} &amp; \\mathbf{aB} &amp; \\mathbf{ab} \\\\ \\hline \\mathbf{AB} &amp; AABB &amp; AABb &amp; AaBB &amp; AaBb \\\\ \\mathbf{Ab} &amp; AABb &amp; AAbb &amp; AaBb &amp; Aabb \\\\ \\mathbf{aB} &amp; AaBb &amp; AaBb &amp; aaBB &amp; aaBb \\\\ \\mathbf{ab} &amp; AaBb &amp; Aabb &amp; aaBb &amp; aabb \\end{array}\\] In his experiment for seed color, the F2 generation produced 6022 yellow, and 2001 green seeds. Thus, the ratio of yellow:green was 3.01:1. This ratio is not the exact theoretical ratio of 3:1. A meaningful question would be this: Is the discrepancy appeared because of random fluctuation, or is the observed ratio significantly different from 3:1? To examine whether the observed count fits a theoretical ratio, we will use the goodness-of-fit test. chisq.test(x = c(6022, 2001), # the observed data p = c(0.75, 0.25)) # the theoretical probability ## ## Chi-squared test for given probabilities ## ## data: c(6022, 2001) ## X-squared = 0.014999, df = 1, p-value = 0.9025 A p-value of 0.9025 suggested a good match of observed data with the theoretical values. That is, the differences are not significant. Let’s assume Mendel had observed a 1000 times larger number of seeds, with the same proportion. That is, 6,022,000 yellow and 2,001,000 green. Obviously this ratio is still 3.01:1. Would it still be a good fit for the theoretical value? chisq.test(x = c(6022000, 2001000), # The observed data, 1000 times larger p = c(0.75, 0.25)) # The theoretical probability ## ## Chi-squared test for given probabilities ## ## data: c(6022000, 2001000) ## X-squared = 14.999, df = 1, p-value = 0.0001076 This time, p = 0.0001076, suggesting a significant deviation from the theoretical ratio. As an extension of CLT, when you sample a large enough sample, the ratio of the categories should approach the true value. In other words, \\(\\chi^2\\) test should be increasingly sensitive to small deviations when the sample size increases. 10.2.2 \\(\\chi^2\\) test for independence In another experiment, Mendel looked at two pairs of phenotypes of the F2 generation of a double-heterozygote. Below is what he saw: 315 round and yellow, 101 wrinkled and yellow, 108 round and green, 32 wrinkled and green. Before we examine the 9:3:3:1 ratio, we want to ask if the two loci are independent of each other. That is, will being yellow increase or decrease the chance of being round (and vice versa)? To run the \\(\\chi^2\\) test for independence, we will first need a contingency table. This time we will manually build a data frame for this purpose. Contingency tables Let’s come back to the data of Asia and Africa in 1952. Take a look at the distribution of the life expectancy for all countries in both continents. summary(As.Af) ## country continent year lifeExp ## Afghanistan: 1 Africa :52 Min. :1952 Min. :28.80 ## Algeria : 1 Americas: 0 1st Qu.:1952 1st Qu.:37.00 ## Angola : 1 Asia :33 Median :1952 Median :40.54 ## Bahrain : 1 Europe : 0 Mean :1952 Mean :41.92 ## Bangladesh : 1 Oceania : 0 3rd Qu.:1952 3rd Qu.:45.01 ## Benin : 1 Max. :1952 Max. :65.39 ## (Other) :79 ## pop gdpPercap ## Min. : 60011 Min. : 298.9 ## 1st Qu.: 1022556 1st Qu.: 684.2 ## Median : 3379468 Median : 1077.3 ## Mean : 19211739 Mean : 2783.3 ## 3rd Qu.: 8550362 3rd Qu.: 1828.2 ## Max. :556263527 Max. :108382.4 ## Notice that the median of life expectancy is 40.54. That is, half of the countries had life expectancy greater than 40.54, and the other half less than 40.54. Let’s define a catagorical variable: the countries with life expectancy &gt; 40.54 years are “longer_lived,” and the others are “shorter_lived.” As.Af[&quot;long_short&quot;] &lt;- NA As.Af$long_short[As.Af$lifeExp &gt; 40.54] &lt;- &quot;longer_lived&quot; As.Af$long_short[is.na(As.Af$long_short)] &lt;- &quot;shorter_lived&quot; Now let’s see if the longer lived or shorter lived variable is independent of the continent variable. We realize that both variables are categorical. In this case, we will use chi-squared test. First, we will make a contingency table of the two variables. As.Af &lt;- droplevels(As.Af) cont &lt;- table(As.Af$continent, As.Af$long_short) cont ## ## longer_lived shorter_lived ## Africa 21 31 ## Asia 22 11 Here, our null hypothesis is that countries are independent of the continent it’s a part of. Likewise, our alternative hypothesis is that countries are dependent (not independent) of the continent it’s a part of. chisq.test(cont) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: cont ## X-squared = 4.5769, df = 1, p-value = 0.03241 Given that \\(p &lt; 0.05\\), our null hypothesis that the two variables are independent is rejected. Whether a country is longer lived or shorter lived is dependent on the continent it is located in. N.B., this is only a comparison between Africa and Asia, and does not hold true for all continents. Mendel2loci &lt;- data.frame( yellow = c(315, 101), green = c(108, 32) ) # adding rownames rownames(Mendel2loci) &lt;- c(&quot;round&quot;, &quot;wrinkled&quot;) # printing the data frame Mendel2loci ## yellow green ## round 315 108 ## wrinkled 101 32 Next we will run the \\(\\chi^2\\) test. The null hypothesis is that the distribution is independent of the groups. chisq.test(Mendel2loci) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: Mendel2loci ## X-squared = 0.051332, df = 1, p-value = 0.8208 The p-value of 0.8208, so we cannot reject the null hypothesis. Therefore, we should consider the two traits as independent. Again, we could try to test for its goodness-of-fit. This time we will not need a contingency table. chisq.test(x = c(315, 101, 108, 32), p = c(9/16, 3/16, 3/16, 1/16)) ## ## Chi-squared test for given probabilities ## ## data: c(315, 101, 108, 32) ## X-squared = 0.47002, df = 3, p-value = 0.9254 Thus, the data fits the 9:3:3:1 ratio well. 10.2.3 \\(\\chi^2\\) test for homogeneity The homogeneity test works the same way as an independence test – the only difference lies in the experimentally design. A test for independence draws samples from the same population, and look at two or more categorical variables. A test for homogeneity draws sample from 2 or more subgroups of the population, and looks at another categorical variable. The subgroup itself serves as a variable. Recall the hypotheses for the test for homogeneity: H\\(_0\\): the distribution of a categorical response variable is the same in each subgroup. H\\(_a\\): the distribution is not the same in each subgroup. Let’s work through a real-life example. div.blue { background-color:#e6f0ff; border-radius: 10px; padding: 20px; } Remdesivir and COVID-19 Remdesivir is an antiviral drug previously tested in animal models infected with coronaviruses like SARS and MERS. As of May 2020, remdesivir had temporary approval from the FDA for use in severely ill COVID-19 patients, and it was the subject of numerous ongoing studies. A randomized controlled trial conducted in China enrolled 236 patients with severe COVID-19 symptoms; 158 were assigned to receive remdesivir and 78 to receive a placebo. In the remdesivir group, 103 patients showed clinical improvement; in the placebo group, 45 patients showed clinical improvement. A placebo is a “fake” treatment. That is, placebos do not contain any active substances that affect health. Reference Wang, Y., Zhang, D., Du, G., Du, R., Zhao, J., Jin, Y., … Wang, C. (2020). Remdesivir in adults with severe COVID-19: a randomised, double-blind, placebo-controlled, multicentre trial. The Lancet. https://doi.org/10.1016/S0140-6736(20)31022-9 If we consider the treatment and the placebo group as two subgroups of the population, we would expect the ratios of clinical improvement to be different. Let’s do a \\(\\chi^2\\) test for homogeneity. We will start with a contingency table. rem_cont &lt;- data.frame(treatment = c(103, 158-103), placebo = c(45, 78-45)) rownames(rem_cont) &lt;- c(&quot;improvement&quot;, &quot;no improvement&quot;) rem_cont ## treatment placebo ## improvement 103 45 ## no improvement 55 33 Next we will run the test. Before we run the test, answer the following questions: What is our null hypothesis? What is our alternative hypothesis? chisq.test(rem_cont) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: rem_cont ## X-squared = 0.95518, df = 1, p-value = 0.3284 What does this result mean? 10.2.4 \\(\\chi^2\\) test assumptions All statistical tests have assumptions for them to accurately represent the data. Here are the assumptions you need to fulfill to use the chi-squared test: 1. Counted data condition: all data must be “counts” for the categorical variable. 2. Independence assumption: the counts in the cells should be independent of each other. (a) Satisfied if the data are from randomly sampled data. 3. Sample size assumption: we should expect to see at least 5 individuals in each cell (recall CLT). (a) If this isn’t satisfied, take your χ2 results with a grain of salt. Example Questions Question 1: A study was conducted and gave the table below. We notice a trend within the data, but how do we quantify it? Note that the numbers are counts. Question: Is the proportion of having heart disease the same for North Americans as it is for Africans? Question 2: Is there evidence that the longevity of an individual is independent of the continent from which the individual comes from? Let’s define long-lived as those who’s life expectancy is greater than 40.54 and short-lived as those less than 40.54. 10.3 Additional resources Fisher’s exact test (OPTIONAL) If the count in any cell of our contigency table is less than 5, the \\(\\chi^2\\) test will not be useful because of its probability distribution assumption. In this case, we will use Fisher’s exact test. The hypotheses of Fisher’s exact same as that of the \\(\\chi^2\\) test. Fisher’s exact test can be used for either homogeneity or independence, depending on your experimental design. Suppose we have a sample 10 times smaller for the Remdesivir trial: small_rem &lt;- data.frame(treatment = c(10, 16-10), placebo = c(4, 8-4)) rownames(rem_cont) &lt;- c(&quot;improvement&quot;, &quot;no improvement&quot;) small_rem ## treatment placebo ## 1 10 4 ## 2 6 4 We have many cells with &lt;5 observations. In this case let’s run Fisher’s exact test. fisher.test(small_rem) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: small_rem ## p-value = 0.6734 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 0.2140763 12.7643113 ## sample estimates: ## odds ratio ## 1.630755 The p-value is 0.6734. What does this result mean? The odds ratio is yet another useful measurement you will often see in medical science articles. For the sake of time, I will leave it to you if you wish to read up on it. Comparison of proportions (OPTIONAL) In the Remdesivir study, the participants were randomly assigned to each group. Thus, the groups can be treated as independent. It is also reasonable to assume independence of patients within each group. Suppose we have two proportions \\(\\hat{p}_1\\) and \\(\\hat{p}_2\\). Then, the normal model can be applied to the difference of the two proportions, \\(\\hat{p}_1 - \\hat{p}_2\\), if the following assumptions are fulfilled: The sampling distribution for each sample proportion is nearly normal. The samples are independent random samples from the relevant populations and are independent of each other. Each sample proportion approximately follows a normal model when \\(n_1p_1\\), \\(n_1(1 - p_1)\\), \\(n_2p_2\\), and \\(n_2(1-p_2)\\) are all are \\(\\geq 10\\). To check success-failure in the context of a confidence interval, use \\(\\hat{p}_1\\) and \\(\\hat{p}_2\\). The standard error of the difference in sample proportions is \\[\\sqrt{\\dfrac{p_1(1-p_1)}{n_1} + \\dfrac{p_2(1-p_2)}{n_2}}. \\] For hypothesis testing, an estimate of \\(p\\) is used to compute the standard error of \\(\\hat{p}_1 - \\hat{p}_2\\): \\(\\hat{p}\\), the weighted average of the sample proportions \\(\\hat{p}_1\\) and \\(\\hat{p}_2\\), \\[\\hat{p} = \\dfrac{n_1\\hat{p}_1 + n_2\\hat{p}_2}{n_1 + n_2} = \\dfrac{x_1 + x_2}{n_1 + n_2}. \\] To check success-failure in the context of hypothesis testing, check that \\(\\hat{p}n_1\\) and \\(\\hat{p}n_2\\) are both \\(\\geq 10\\). In this case, let’s calculate the The pooled proportion \\(\\hat{p}\\): \\[\\hat{p} = \\dfrac{x_1 + x_2}{n_1 + n_2} = 0.627\\] x = c(103, 45) n = c(158, 78) p.hat.vector = x/n p.hat.vector ## [1] 0.6518987 0.5769231 # use r as a calculator p.hat.pooled = sum(x)/sum(n) p.hat.pooled ## [1] 0.6271186 Next we will check the success-failure condition, which is, \\(\\hat{p}n_1\\) and \\(\\hat{p}n_2\\) are both \\(\\geq 10\\). # check success-failure n*p.hat.pooled ## [1] 99.08475 48.91525 n*(1 - p.hat.pooled) ## [1] 58.91525 29.08475 The success-failure condition is met; the expected number of successes and failures are all larger than 10. # conduct inference prop.test(x = x, n = n) ## ## 2-sample test for equality of proportions with continuity correction ## ## data: x out of n ## X-squared = 0.95518, df = 1, p-value = 0.3284 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## -0.06703113 0.21698245 ## sample estimates: ## prop 1 prop 2 ## 0.6518987 0.5769231 In this example, we tested \\(H_0: p_1 = p_2\\) against \\(H_a: p_1 \\neq p_2\\) Here, \\(p_1\\) represents the population proportion of clinical improvement in COVID-19 patients treated with remdesivir, and \\(p_2\\) represents the population proportion of clinical improvement in COVID-19 patients treated with a placebo. By convention, \\(\\alpha = 0.05\\). The \\(p\\)-value is 0.3284, which is greater than \\(\\alpha\\). We conclude that there is insufficient evidence to reject the null hypothesis. Although the proportion of patients who experienced clinical improvement about 7% higher in the remdesivir group, this difference is not big enough to show that remdesivir is more effective than a placebo. Visualizing data distributions (OPTIONAL) We’ve barely scratched the surface of dplyr but I want to point out key things you may start to appreciate. dplyr’s verbs, such as filter() and select(), are what’s called pure functions. To quote from Wickham’s Advanced R Programming book: The functions that are the easiest to understand and reason about are pure functions: functions that always map the same input to the same output and have no other impact on the workspace. In other words, pure functions have no side effects: they don’t affect the state of the world in any way apart from the value they return. And finally, the data is always the very first argument of every dplyr function. Knowing the properties of the normal distribution is essential in understanding the normal distribution. The position of the peak indicates the mean, whereas the spread of the curve indicates the variance. Although you might not think your data follows a bell curve, let’s take a look at this example for our exercise. Let’s first install a package that helps us create ridgeline plots. install.packages(&quot;ggridges&quot;) Here we will plot the distribution of the life expectancy of African countries in different years. For each year, distributions are sectioned into quartiles. What could you say about the trend over the years? Please discuss both the mean and variance. What does it mean? library(ggridges) # getting all rows with Africa as the continent Africa.all &lt;- gapminder %&gt;% filter(continent == &quot;Africa&quot;, year &gt; 1990) # plotting p &lt;- ggplot(Africa.all, aes(lifeExp, as.factor(year), fill=factor(stat(quantile)))) + stat_density_ridges(quantiles=4, quantile_lines=T, geom = &#39;density_ridges_gradient&#39;) + scale_fill_viridis_d(name=&#39;Quartile&#39;) + labs(x=&#39;Life expectancy (yrs)&#39;, y=&#39;Year&#39;) + theme_classic() show(p) ## Picking joint bandwidth of 3.6 Let’s take a look at the mean and standard deviation to see if your guess is correct. Africa.all %&gt;% select(c(year, lifeExp)) %&gt;% group_by(as.factor(year)) %&gt;% summarize(mean_life = mean(lifeExp), sd_life = sd(lifeExp)) ## # A tibble: 4 x 3 ## `as.factor(year)` mean_life sd_life ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1992 53.6 9.46 ## 2 1997 53.6 9.10 ## 3 2002 53.3 9.59 ## 4 2007 54.8 9.63 This visualization shows the same information as that in with the density plots, but in a more digestible manner. p &lt;- ggplot(data = Africa.all, aes(x=as.factor(year), y=lifeExp)) + geom_dotplot(binaxis = &#39;y&#39;, stackdir = &#39;center&#39;, binwidth = 1) + geom_boxplot(alpha=0.3) show(p) You may want to remove the gray background and decrease dot size. This is as easy as specifying the dotsize parameter and adding theme_classic(). There a lot more themes out there! Check them out here. p &lt;- ggplot(data = Africa.all, aes(x=as.factor(year), y=lifeExp)) + geom_dotplot(binaxis = &#39;y&#39;, stackdir = &#39;center&#39;, binwidth = 1, dotsize = 0.65) + geom_boxplot(alpha=0.3) + theme_classic() show(p) While we’re at it, let’s also rename the x- and y-axis. Since we’ve saved the plot already, let’s add a label layer to the plot. p &lt;- p + labs(x = &#39;Year&#39;, y = &#39;Life expectancy (yrs)&#39;) show(p) Now this figure is publication-ready. "],["comparing-multiple-means.html", "11 Comparing multiple means 11.1 ANOVA in a nutshell 11.2 One-way ANOVA 11.3 Beyond the one-way ANOVA", " 11 Comparing multiple means Throughout this chapter, we will provide a pipeline to help you wrangle data, perform statistical analyses, and (perhaps most importantly) visualize data in R. Let’s load the usual packages, and one new package called car (Companion to Applied Regression). library(gapminder) library(tidyverse) library(car) # for Levene&#39;s test 11.1 ANOVA in a nutshell Analysis of variance (ANOVA). ANOVA tells us whether multiple groups are significantly different from each other. We can write our general hypotheses as this: H\\(_0\\): there is no difference between the groups. H\\(_a\\): there is a difference between the groups. Like how the t-test used the t statistic, ANOVA uses the F statistic. In general: A small F means there likely is not a difference between the groups (H0). A large F means there likely is a difference between the groups (Ha). ANOVA assumes that All groups have equal variance, and All groups have a normal distribution. To check the assumptions, first visualize your data! This will provide a sanity check for the results from statistical tests. Now for the tests: You can use var.test() or leveneTest() to check for equal variance. You can use shapiro.test() to check for a normal distribution. An important remark is that ANOVA does not tell you which groups are significantly different, only whether there are significant differences between your groups. Here is a pipeline for running ANOVA: Visualize your data. Side-by-side boxplots are a good bet. Check for equal variance and normality. Run the ANOVA. If results are not significant, conclude that there there are no significant difference between your groups. If results are significant, use a post-hoc test to see which groups are significantly different (more on this later). 11.2 One-way ANOVA 11.2.1 The iris example The iris dataset contains information about three species of flowers: setosa, veriscolor, and virginia. Iris is a built-in dataset in R, meaning we can call it without reading it in. iris$Species refers to one column in iris. That is, the column with the name of the species (setosa, versicolor, or virginica). We can see how many rows and columns are in a data.frame with the dim command. dim(iris) prints out the number of rows (150) and the number of columns (5): head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa Analysis of Variance (ANOVA) allows us to test whether there are differences in the mean between multiple samples. The question we will address is: Are there differences in average sepal width among the three species? As good data scientists, we should first visualize our data. ggplot(iris, aes(Sepal.Length, col=Species)) + geom_boxplot() + theme_classic() To run an ANOVA, we need to check if The variance is is equal for each group, and The data distributes normally within each group. Let’s address the first point. Before doing a test, let us look at the figure above. The variance in each group looks relatively equal. Keep this in mind as we interpret Levene’s test for equal variances. leveneTest(Sepal.Width ~ Species, data = iris) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 2 0.5902 0.5555 ## 147 What does the p-value suggest? Solution A p-value of 0.5555 suggested that the variances are not significantly different. With this assumption checked, we will proceed with ANOVA. Keep in mind we haven’t yet checked the normality. We will do it after running ANOVA. We start by building an ANOVA model with the aov() function. ANOVA. To run ANOVA in R, we need to pass two arguments to the aov() function. In general, this is what an ANOVA call looks like: aov(y ~ x, data) For the formula parameter, the format is y ~ x, where the response variable (y) is to the left of the tilde (~), and the predictor variable (x) is to the right of the tilde. For the data parameter, we place the naame of the dataset we want to analyze. In this example, we are asking if petal length is significantly different among the three species. We also need to tell R where to find the Sepal.Width and Species data, so we pass the variable name of the iris data.frame to the data parameter. Let’s build the model: Sepal.Width.aov &lt;- aov(formula = Sepal.Width ~ Species, data = iris) Notice how when we execute this command, nothing printed in the console. This is because we instead sent the output of the aov call to a variable. If you just type the variable name, you will see the familiar output from the aov function: Sepal.Width.aov ## Call: ## aov(formula = Sepal.Width ~ Species, data = iris) ## ## Terms: ## Species Residuals ## Sum of Squares 11.34493 16.96200 ## Deg. of Freedom 2 147 ## ## Residual standard error: 0.3396877 ## Estimated effects may be unbalanced To see the results of the ANOVA, we call the summary() function: summary(Sepal.Width.aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Species 2 11.35 5.672 49.16 &lt;2e-16 *** ## Residuals 147 16.96 0.115 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Answer to our question part 1: The species do have significantly different sepal width (P &lt; 0.001). However, ANOVA does not tell us which species are different. We can run a post hoc test to assess which species are different. A Tukey test comparing means would be one option. We will run the Tukey test after determining normality. Evaluating normality Now, let’s take a look at the normality. Once again, let’s visualize our data before running a test: ggplot(iris, aes(Sepal.Length, fill=Species)) + geom_density(alpha=0.3) + theme_classic() From the visualization alone, each species’ distribution of sepal lengths looks normal. Now, we will plot the diagnostic figures. plot(Sepal.Width.aov) Most importantly, the residuals in Q-Q plot (upper right) should align with the line pretty well. If the residuals were to deviate from the line too much, the data would not be considered normal. If you still perform the ANOVA, you should view your results critically (or ignore them, at worst). Recall that a residual is an “error” in result. More specifically, a residual is the difference of a given data point from the mean (\\(x - \\mu\\)). Please do not include diagnostic figures in the main text of your manuscripts. This might qualify for a supplementary figure at most. Although we’ve also examined residuals with the Q-Q plot, we can also use a formal test: residuals_Sepal_Width &lt;- residuals(object = Sepal.Width.aov) shapiro.test(x = residuals_Sepal_Width) ## ## Shapiro-Wilk normality test ## ## data: residuals_Sepal_Width ## W = 0.98948, p-value = 0.323 A p-value of 0.323 suggested that the assumption of normality is reasonable. So far, we have demonstrated Normality in distribution and Homogeneity in variance. These two justified our choice for one-way ANOVA. The result of ANOVA also indicated that at least one species of the 3 has significantly different sepal width from others. Which ones are significantly different? To do this, we need to run post-hoc test. Let’s do Tukey Honest Significant Differences (HSD) test. The nice thing is that TukeyHSD() can directly take the result of ANOVA as the argument. TukeyHSD(Sepal.Width.aov) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = Sepal.Width ~ Species, data = iris) ## ## $Species ## diff lwr upr p adj ## versicolor-setosa -0.658 -0.81885528 -0.4971447 0.0000000 ## virginica-setosa -0.454 -0.61485528 -0.2931447 0.0000000 ## virginica-versicolor 0.204 0.04314472 0.3648553 0.0087802 Answer to our question part 2: The difference between every species of flower is significant at \\(\\alpha = 0.05\\). 11.2.2 Non-parametric alternatives to ANOVA In reality, your data usually will not fulfill every assumption for an ANOVA. In case of a non-normal sample, there are two ways to address the problem: Apply appropriate data transformation techniques, or Use a non-parametric test. I highly recommend you to explore data transformation. If you can rescue your data back to normal distribution, parametric tests usually allow you to do more powerful analysis. If you have exhausted your attempts to data transformation, you may then use non-parametric tests. When your data doesn’t satisfy the normality or equal variance assumption, ANOVA is not the best test to use. The Kruskal-Wallis test doesn’t assume normality, but it does assume same distribution across groups (equal variance). If your data do not meet either assumption, you would want to use Welch’s one-way test. Now, let’s get back to gapminder data. First, add another categorical variable calle Income_Level. This time we will split by the quartiles. dat.1952 &lt;- gapminder %&gt;% filter(year == 1952) border_1952 &lt;- quantile(dat.1952$gdpPercap, c(.25, .50, .75)) dat.1952$Income_Level_1952 &lt;- cut(dat.1952$gdpPercap, c(0, border_1952[1], border_1952[2], border_1952[3], Inf), c(&#39;Low&#39;, &#39;Low Middle&#39;, &#39;High Middle&#39;, &#39;High&#39;)) dat.2007 &lt;- gapminder %&gt;% filter(year == 2007) border_2007 &lt;- quantile(dat.2007$gdpPercap, c(.25, .50, .75)) dat.2007$Income_Level_2007 &lt;- cut(dat.2007$gdpPercap, c(0, border_2007[1], border_2007[2], border_2007[3], Inf), c(&#39;Low&#39;, &#39;Low Middle&#39;, &#39;High Middle&#39;, &#39;High&#39;)) head(dat.1952) ## # A tibble: 6 x 7 ## country continent year lifeExp pop gdpPercap Income_Level_1952 ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. Low ## 2 Albania Europe 1952 55.2 1282697 1601. Low Middle ## 3 Algeria Africa 1952 43.1 9279525 2449. High Middle ## 4 Angola Africa 1952 30.0 4232095 3521. High Middle ## 5 Argentina Americas 1952 62.5 17876956 5911. High ## 6 Australia Oceania 1952 69.1 8691212 10040. High For now, let’s focus on the data for the year 1952. ggplot(data = dat.1952, aes(x = Income_Level_1952, y = lifeExp)) + geom_boxplot() + theme_classic() Let’s check the variance. leveneTest(lifeExp ~ Income_Level_1952, data = dat.1952) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 3 4.3637 0.005714 ** ## 138 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 A p-value of 0.0047 suggested that the variances are significantly different. Therefore, we shoud not run ANOVA or Kruskal-Wallis. Let’s run Welch’s one-way test. result &lt;- oneway.test(lifeExp ~ Income_Level_1952, data = dat.1952) result ## ## One-way analysis of means (not assuming equal variances) ## ## data: lifeExp and Income_Level_1952 ## F = 69.799, num df = 3.000, denom df = 73.647, p-value &lt; 2.2e-16 What does the p-value mean? Solution A p-value of 2.2e-16 suggested that at least one category of Income_Level_1952 had values of lifeExp that are significantly different from others. Let’s run a post-hoc test to find out which categories are significant. Since we are running a non-parametric test, an appropriate test would be Games-Howell post-hoc test. Unfortunately, R does not have a built-in function for Games-Howell. Let’s define a function to do this task. Games-Howell test Note: you don’t need to know how the code below works. games.howell &lt;- function(grp, obs) { # Create combinations combs &lt;- combn(unique(grp), 2) # Statistics that will be used throughout the calculations: # n = sample size of each group # groups = number of groups in data # Mean = means of each group sample # std = variance of each group sample n &lt;- tapply(obs, grp, length) groups &lt;- length(tapply(obs, grp, length)) Mean &lt;- tapply(obs, grp, mean) std &lt;- tapply(obs, grp, var) statistics &lt;- lapply(1:ncol(combs), function(x) { mean.diff &lt;- Mean[combs[2,x]] - Mean[combs[1,x]] # t-values t &lt;- abs(Mean[combs[1,x]] - Mean[combs[2,x]]) / sqrt((std[combs[1,x]] / n[combs[1,x]]) + (std[combs[2,x]] / n[combs[2,x]])) # Degrees of Freedom df &lt;- (std[combs[1,x]] / n[combs[1,x]] + std[combs[2,x]] / n[combs[2,x]])^2 / # numerator dof ((std[combs[1,x]] / n[combs[1,x]])^2 / (n[combs[1,x]] - 1) + # Part 1 of denominator dof (std[combs[2,x]] / n[combs[2,x]])^2 / (n[combs[2,x]] - 1)) # Part 2 of denominator dof # p-values p &lt;- ptukey(t * sqrt(2), groups, df, lower.tail = FALSE) # sigma standard error se &lt;- sqrt(0.5 * (std[combs[1,x]] / n[combs[1,x]] + std[combs[2,x]] / n[combs[2,x]])) # Upper Confidence Limit upper.conf &lt;- lapply(1:ncol(combs), function(x) { mean.diff + qtukey(p = 0.95, nmeans = groups, df = df) * se })[[1]] # Lower Confidence Limit lower.conf &lt;- lapply(1:ncol(combs), function(x) { mean.diff - qtukey(p = 0.95, nmeans = groups, df = df) * se })[[1]] # Group Combinations grp.comb &lt;- paste(combs[1,x], &#39;:&#39;, combs[2,x]) # Collect all statistics into list stats &lt;- list(grp.comb, mean.diff, se, t, df, p, upper.conf, lower.conf) }) # Unlist statistics collected earlier stats.unlisted &lt;- lapply(statistics, function(x) { unlist(x) }) # Create dataframe from flattened list results &lt;- data.frame(matrix(unlist(stats.unlisted), nrow = length(stats.unlisted), byrow=TRUE)) # Select columns set as factors that should be numeric and change with as.numeric results[c(2, 3:ncol(results))] &lt;- round(as.numeric(as.matrix(results[c(2, 3:ncol(results))])), digits = 3) # Rename data frame columns colnames(results) &lt;- c(&#39;groups&#39;, &#39;Mean Difference&#39;, &#39;Standard Error&#39;, &#39;t&#39;, &#39;df&#39;, &#39;p&#39;, &#39;upper limit&#39;, &#39;lower limit&#39;) return(results) } After defining the function, we can use it. If you decide to use the Games-Howell function, you can simply copy-and-paste it. games.howell(grp = dat.1952$Income_Level_1952, # groups, the categorical variable obs = dat.1952$lifeExp) # observations, the continuous variable ## groups Mean Difference Standard Error t df p ## 1 Low : Low Middle 5.569 1.073 3.671 59.455 0.003 ## 2 Low : High Middle 13.243 1.287 7.274 51.265 0.000 ## 3 Low : High 24.536 1.247 13.917 53.954 0.000 ## 4 Low Middle : High Middle 7.673 1.450 3.743 64.272 0.002 ## 5 Low Middle : High 18.967 1.414 9.487 66.664 0.000 ## 6 High Middle : High 11.294 1.583 5.046 68.787 0.000 ## upper limit lower limit ## 1 9.580 1.559 ## 2 18.077 8.408 ## 3 29.210 19.863 ## 4 13.081 2.266 ## 5 24.235 13.699 ## 6 17.187 5.401 If you’re interested in learning more, check out this article on the Kruskal-Wallis test. 11.3 Beyond the one-way ANOVA As seen in the previous example, a one-way ANOVA is a great tool for comparing multiple groups on one categorical variable such as income level. But what if we wanted to examine if life expectancy differs across the years 1952, 1972, and 2007 in addition to income level? In this case, we will need a two-way ANOVA. Although we won’t cover a two-way ANOVA in this text, you can learn more about it in this article. "],["linear-regression.html", "12 Linear regression 12.1 Basic concepts 12.2 Pearson correlation 12.3 Spearman correlation", " 12 Linear regression Linear regression is a powerful tool that attempts to model the relationship between a dependent and independent variable by fitting a linear equation to observed data. In particular, the model attempts to minimize the distance between actual and predicted y-values, called residuals. To get an intuition for linear regression, try playing around with the application below. 12.1 Basic concepts Let’s load the usual packages. library(gapminder) library(tidyverse) library(car) # car stands for Companion to Applied Regression In this section, we will test for a relationship between life expectancy and per capita gross domestic product (GDP). As with all statistical tests, it is a good idea to visualize our data when possible. Let’s create a scatterplot of the two columns of interest, lifeExp and gdpPercap. dat.2007 &lt;- gapminder %&gt;% filter(year == 2007) ggplot(dat.2007, aes(x = gdpPercap, y = lifeExp)) + geom_point() We can see immediately that this is unlikely a linear relationship. In this case, we will need to log-transform the GDP data to obtain a linear relationship. ggplot(dat.2007, aes(x = gdpPercap, y = lifeExp)) + geom_point() + scale_x_log10() Now that the data are properly transformed, we can create the linear model for the predictability of life expectancy based on gross domestic product. Before we do that let’s make it clear: From the scatter plot we can identify a positive relationship – when log(GDP per capita) increases, the life expectancy also tends to be higher. The tendency of one variable going up or down linearly with the increase of another variable is called “correlation.” The more consistent the points are with a LINEAR trend, the higher the closer the correlation is to -1 (for negative relationships) or +1 (for positive relationships). How fast one variable increases or decreases with the increase of another variable can be described by the slope of the fitting line. To estimate the slope, we need a linear model. We can only discuss strength of correlation with these linear regression, but NOT the causation. That is, correlation does NOT imply causation. We can plot the linear model easily: ggplot(data = dat.2007, aes(x = gdpPercap, y = lifeExp)) + geom_point() + scale_x_log10() + geom_smooth(method = &quot;lm&quot;) # lm = linear model ## `geom_smooth()` using formula &#39;y ~ x&#39; To get rid of the confidence band around the line, pass se = FALSE into geom_smooth(). ggplot(data = dat.2007, aes(x = gdpPercap, y = lifeExp)) + geom_point() + scale_x_log10() + geom_smooth(method = &quot;lm&quot;, se = FALSE) You can also customize the colour and thickness of the line. Use the ? operator or read the documentation to see additional options. Here is another interactive app showing how different data affects the regression model. 12.2 Pearson correlation Let’s look at the correlation. For normal distributed data, we calculate the Pearson correlation for the log-transformed variable. dat.2007$log_GDP &lt;- log(dat.2007$gdpPercap) # add a new variable lifeExp.v.gdp &lt;- lm(formula = lifeExp ~ log_GDP, data = dat.2007) summary(lifeExp.v.gdp) ## ## Call: ## lm(formula = lifeExp ~ log_GDP, data = dat.2007) ## ## Residuals: ## Min 1Q Median 3Q Max ## -25.947 -2.661 1.215 4.469 13.115 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.9496 3.8577 1.283 0.202 ## log_GDP 7.2028 0.4423 16.283 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.122 on 140 degrees of freedom ## Multiple R-squared: 0.6544, Adjusted R-squared: 0.652 ## F-statistic: 265.2 on 1 and 140 DF, p-value: &lt; 2.2e-16 The linear equation is: \\(\\text{lifeExp} = (7.1909 \\pm 0.4602) \\times \\text{log_GDP} + (4.7951 \\pm 4.0045)\\). Also notice that the correlation coefficient is \\(R^2 = 0.6526 \\Rightarrow R = \\sqrt{0.6526} = 0.8078\\). The p-value suggests the correlation is significant. The correlation coefficient of 0.8 suggests a positive correation (y increases as x increases). In case you see a negative value, the correlation if negative (one variable going up while the other going down). For our question, the relationship between life expectancy and GDP, focus on the coefficients section, specifically the line for log_GDP. First of all, there is a significant relationship between these two variables (p &lt; 2 x 10-16, or, as R reports in the Pr&gt;(|t|) column, p &lt; 2e-16). The Estimate column of the results lists a value of lifeExp.v.gdp$coefficients['log_GDP']. For every 10-fold increase in per capita GDP (remember we log10-transformed GDP), life expectancy increases by almost 7 years. The linear model assumes that your data is normally distributed for each independent observation. We can generate a diagnostic plot in the same way as one-way ANOVA. plot(lifeExp.v.gdp) Q-Q plot suggested this data deviates from normality. Let’s also take a look at the residues of the linear model: residuals_lm &lt;- residuals(object = lifeExp.v.gdp) shapiro.test(x = residuals_lm) ## ## Shapiro-Wilk normality test ## ## data: residuals_lm ## W = 0.90029, p-value = 2.742e-08 Indeed, Shapiro test also suggests the data deviates from normality. In this case, we should use the Spearman (or Kendall) correlation. 12.3 Spearman correlation If your variables are not normally distributed, you can use the non-parametric Spearman correlation as alternative. Instead of Pearson’s R, the Spearman test outputs rho (\\(\\rho\\)), cor.test(dat.2007$lifeExp, dat.2007$log_GDP, method = &quot;spearman&quot;) ## ## Spearman&#39;s rank correlation rho ## ## data: dat.2007$lifeExp and dat.2007$log_GDP ## S = 68434, p-value &lt; 2.2e-16 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.8565899 "],["data-visualization-basics.html", "13 Data visualization basics 13.1 A graphing template 13.2 Scatter plot 13.3 Aesthetic mappings", " 13 Data visualization basics “The simple graph has brought more information to the data analyst’s mind than any other device.” – John Tukey This lab is all about preparing publication-ready figures with ggplot2 and related packages. ggplot2 uses elegant syntax and it implements “The Layered Grammar of Graphics”. Before we begin, let’s review a few key points for good figures: Be clear and avoid confusion. Presenting too much information often results in messy figures. Figures inconsistent in colour, symbols, etc. can easily confuse readers. Only use additional aesthetic effects when necessary. Everything in a graph has a purpose. The primary objective of a figure is to inform, not to look fancy (though this is a plus). When in doubt, stick to black, white, and grey. Only use texts when necessary. Make text large. Never use Comic Sans as your font. Sans serif fonts such as Arial and Calibri are usually good bets. Key point: the plot depends on the variables. Some plots are more appropriate for visualization than others. You have an obligation to display data responsibly. Check out The R Graph Gallery for a “dictionary” on different visualizations and the code to create them. Recall that the tidyverse contains ggplot2 and dplyr among other packages. We’ll also load gapminder. library(gapminder) library(tidyverse) Hopefully by now you have a good idea of what the gapminder dataset looks like. Here’s a quick refresher: head(gapminder) ## # A tibble: 6 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. 13.1 A graphing template Although we’ve presented graphs with ggplot2 in previous labs, let’s delve into the specifics of the syntax: ggplot(data = &lt;DATA&gt;) + &lt;GEOM_FUNCTION&gt;(mapping = aes(&lt;MAPPINGS&gt;)) Everything in the initial ggplot() function is passed into the subsequent functions (i.e., GEOM_FUNCTION()). When calling ggplot(), you don’t explicitly need to write &lt;ARGUMENT&gt; = (ex: data = gapminder, x = lifeExp) as long as you have the variables in the correct order – just be careful you don’t mix up x and y! ggplot2 works in a layer-by-layer manner. Take a look at this: ggplot(data = gapminder, aes(x = pop, y = gdpPercap)) ggplot(data = gapminder, aes(x = pop, y = gdpPercap)) + geom_point() Here, the first line initializes the object and the second line adds a layer of scatter points. Unlike plotting in base R, you don’t need to specify variables using the $ operator – ggplot2 is smart enough to call it automatically for you. I’d like to bring your attention to the + operator. This is how you add layers to the ggplot. Use + liberally to reduce excessively long lines; breaking long commands at appropriate places makes your code much more readable. 13.2 Scatter plot Use the scatter plot when you want to see the relationship between two continuous variables. ggplot(gapminder, aes(gdpPercap, lifeExp)) + geom_point() You also can save the object as a variable. To show the figure, use the show() function or simply call the object by its name. p &lt;- ggplot(gapminder, aes(gdpPercap, lifeExp)) + geom_point() show(p) # method 1 p # also works This data might benefit using a log scale. You can either log-transform (log(gdpPercap)) or simply draw the x axis in log scale (scale_x_log10()). p &lt;- ggplot(gapminder, aes(gdpPercap, lifeExp)) + geom_point() + scale_x_log10() show(p) 13.2.1 Trend lines It seems that there exists a positive correlation between the two variables – you might want to add a trend line. Remember, p is the object of scatter plot with x in log. We can just build from here. This just goes to show the beauty of layered graphical syntax. p + geom_smooth() ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; You find that R used ‘gam’ model as default. ‘gam’ is the generalized additive model. Without going into the mathematical details, you would expect a curve from gam. What if I want a straight line (i.e., linear regression)? I would want a straight line to start with. Let’s start with a linear model (lm). p + geom_smooth(method = &#39;lm&#39;) ## `geom_smooth()` using formula &#39;y ~ x&#39; We can also create a generalized linear model (glm). glm can be useful if your variables are not normally distributed. p + geom_smooth(method = &#39;glm&#39;) ## `geom_smooth()` using formula &#39;y ~ x&#39; It appears thatlm and glm don’t look too different. Notice that lm has a little shaded region around it. This correspondends to the confidence interval of 1 standard error. To get rid of it, specify se = FALSE. While we’re at it, let’s change the colour of the line to red and decrease its thickness. Let’s also remove that pesky grey background: scatter_trend &lt;- p + theme_classic() + # removes grey background geom_smooth(method = &#39;lm&#39;, se = F, # remove confidence band col = &#39;red&#39;, # change colour of line to red (hex colours also work: #FF0000) size = 0.75) # set width of line show(scatter_trend) ## `geom_smooth()` using formula &#39;y ~ x&#39; 13.2.2 Facets If you were paying careful attention, you may have noticed that we were using data from multiple years. However, this results in a messy, and potentially misleading, graph. Let’s fix this by plotting each year separately with the facet feature. ggplot(data = gapminder, aes(x = gdpPercap, y = lifeExp)) + facet_wrap(vars(year), nrow = 3, ncol = 4) + geom_point(size = 0.5) + scale_x_log10() + theme_bw() # another ggplot2 theme We can use faceting to split by the combination of two variables. Here I will use facet_grid to put the same values of splitting variables on the same row/column. Note that you also need to specify vars(year) and vars(continent) instead of just year and continent. This is simply to help ggplot retrieve the levels in a particular column. gap.52.77.07 &lt;- gapminder %&gt;% filter(year %in% c(1952, 1977, 2007)) ggplot(data = gap.52.77.07, aes(x = gdpPercap, y = lifeExp)) + facet_grid(rows = vars(year), cols = vars(continent)) + geom_point() + scale_x_log10() + theme_bw() 13.3 Aesthetic mappings A scatter plot places dots using x and y coordinates. What if we want to show more detail, like which point(s) correspond to a particular group? For example, what if we want to see how life expectancy vs GDP per capita varies per continent in 1977? gap.77 &lt;- gapminder %&gt;% filter(year == 1977) ggplot(data = gap.77, aes(x = gdpPercap, y = lifeExp)) + geom_point(aes(color=continent)) + scale_x_log10() You can see the points from some continents, like Europe and Africa, cluster at distinct positions. In addition to adding colours to show (categorical or continuous) groupings, we can also use Shape of points (categorical), Size of points (continuous), or Transparency/alpha of the points (continuous). These options are all specified within aesthetic mappings (aes()). That is, aes() is the place you specify how you present your variables. More specifically, it’s how you map your variables to various aesthetics. To repeat an earlier point, aes() within the ggplot() function applies to ALL layers, while those in other layers only applies to that specific layer. Here’s another example. Be careful with this as R interprets year as a continuous variable. Use as.factor() or factor() to circumvent this issue, ggplot(data = gap.52.77.07, aes(x = gdpPercap, y = lifeExp)) + geom_point(aes(col = as.factor(year))) + scale_x_log10() Going back to our original example of lifeExp vs gdpPercap by continent, I use stat_ellipse() to enclose the points within a 95% confidence interval. ggplot(data = subset(gapminder, year == 1977), aes(x = gdpPercap, y = lifeExp)) + # color only applies to the points, not the eclipses geom_point(aes(color=continent)) + # stat_ellipse uses level=0.95 by default stat_ellipse() + scale_x_log10() We can also create multiple ellipses to group things together. Note that color = continent is the same as colour = continent and col = continent. ggplot(data = subset(gapminder, year == 1977), aes(x = gdpPercap, y = lifeExp, color = continent)) + # color applies to both points and eclipses geom_point() + stat_ellipse() + scale_x_log10() ## Too few points to calculate an ellipse ## Warning: Removed 1 row(s) containing missing values (geom_path). What if we want to also visualize population size in addition to grouping by continent? ggplot(data = subset(gapminder, year == 1977), aes(x = gdpPercap, y = lifeExp)) + geom_point(aes(color=continent, size=pop)) + scale_x_log10() Here we run into a minor problem: some dots are overlapping. We can fix this by applying geom_jitter() with partial transparency. ggplot(data = subset(gapminder, year == 1977), aes(x = gdpPercap, y = lifeExp)) + geom_jitter(alpha=0.7, aes(color=continent, size=pop)) + scale_x_log10() Notice that geom_jitter() adds some random variation, or jitter, to each point. While this is a handy method to address overplotting, don’t rely on it too heavily. This is illustrated in the next plot: ggplot(data = gapminder, aes(x = gdpPercap, y = lifeExp)) + geom_jitter(aes(color=continent, size=pop, alpha=year)) + scale_x_log10() As you can see, this is a fancy figure, but also a messy figure. This plot has “information overload,” so we would like to simplify it. To reduce the amount of information, we come back to facets. Be careful with this one as R interprets year as a continuous variable. Use as.factor() or factor() to circumvent this issue, ggplot(data = gapminder, aes(x = gdpPercap, y = lifeExp)) + geom_jitter(alpha=0.65, aes(size=pop, color=as.factor(year))) + facet_wrap(vars(continent)) + scale_x_log10() Finally, we can change the labels and text formats. Here, we can save the figure to a .png format. Alternatively, you can save the figure using the Export tab in the Plots viewing panel in RStudio. p &lt;- ggplot(data = gapminder, aes(x = gdpPercap, y = lifeExp)) + geom_jitter(alpha=0.65, aes(size=pop, color=as.factor(year))) + facet_wrap(vars(continent)) + scale_x_log10() + labs(x = &quot;GDP per capita&quot;, y = &quot;Life Expectancy&quot;, size = &quot;Population&quot;, color = &quot;Year&quot;) + theme_bw() + # remove the gray background theme(text = element_text(size = 16)) # make texts larger show(p) ggsave(&quot;data/09_ggplot2/Life_GDP.png&quot;, plot = p) ## Saving 7 x 5 in image "],["getting-publication-ready.html", "14 Getting publication-ready 14.1 Line plot 14.2 Bar plot 14.3 Box plot 14.4 Histogram and density plot", " 14 Getting publication-ready As the chapter name suggests, this chapter is all about getting publication-quality plots. library(tidyverse) library(gapminder) 14.1 Line plot A line plot is another way to visualize continuous variables. This is particularly useful when Your observations change over time and You want to demonstrate a causal relationship. Let’s elaborate on the second point. In the previous case for life expectancy and GDP, we could only observe a correlation, but cannot conclude a causal relationship. In some case, such as carrying out a laboratory experiment or a simulation study, you can precisely manipulate certain independent variables and measure other dependent variables. This way you can argue for a better causal relationship. For example, you can change the concentration of a drug treatment and measure the inhibition effect. Let’s take a look at the life expectancy of Africa over the years. Let’s first create a scatter plot. ggplot(data = subset(gapminder, continent == &quot;Europe&quot;), aes(x = year, y = lifeExp)) + geom_point() Since the points can correspond to the countries, we can connect them with lines. ggplot(data = subset(gapminder, continent == &quot;Europe&quot;), aes(x = year, y = lifeExp, colour = country)) + geom_point() + geom_line(aes(group = country)) Here the group = country specifies that points with the same values for the variable country should be connected in a line. Oftentimes you want to show line plots with mean values and error bars. Unfortunately, ggplot2 can’t automatically draw error bars – you have to explicitly specify the values. We’re going to address this in the next example. First, calculate the mean and SEM and save it to a new data frame. df &lt;- gapminder %&gt;% group_by(year, continent) %&gt;% summarise(mean_le = mean(lifeExp), sd=sd(lifeExp), sem = sd(lifeExp)/sqrt(n())) ## `summarise()` has grouped output by &#39;year&#39;. You can override using the `.groups` argument. Next, draw a line plot with this data frame. To draw error bars, we need to specify the upper and lower limits within geom_errorbar(). lineplot &lt;- ggplot(data = df, aes(x = year, y = mean_le, color = continent)) + geom_line() + geom_point() + geom_errorbar(aes(ymin = mean_le-sem, ymax = mean_le+sem), position = position_dodge(0.05)) # position_dodge() sets length of error bar caps show(lineplot) Finally, let’s format the figure nicely. lineplot &lt;- lineplot + labs(x = &quot;Year&quot;, y = &quot;Life Expectancy&quot;, color = &quot;Continent&quot;) + theme_classic() + # remove the gray background theme(text = element_text(size = 16)) # set font size show(lineplot) For a cleaner view with offset axis: lineplot &lt;- lineplot + theme(axis.line = element_blank(), # hide the default axes axis.ticks.length = unit(7, &#39;pt&#39;)) + # specify the breaks of y-axis scale_y_continuous(breaks=seq(40,80,20), limits=c(35,85), expand=c(0,0)) + # specify the breaks of x-axis scale_x_continuous(breaks=seq(1950, 2010, 20), limits=c(1945,2010), expand=c(0,0)) + # specify the location of the new y-axis geom_segment(y=40, yend=80, x=1945, xend=1945, lwd=0.5, colour=&quot;black&quot;, lineend=&quot;square&quot;) + # specify the location of the new x-axis geom_segment(y=35, yend=35, x=1950, xend=2010, lwd=0.5, colour=&quot;black&quot;, lineend=&quot;square&quot;) show(lineplot) ggsave(&quot;data/10_pubvis/life_year.png&quot;, plot = lineplot) ## Saving 7 x 5 in image 14.2 Bar plot 14.2.1 Basics Let’s look at the life expectancy of all continents in 1977 using a bar plot. Notice that instead of using dplyr, we can simply use the base R subset() function. Recall df from the previous section. ggplot(data = subset(df, year == 1977), aes(x=continent, y=mean_le)) + geom_col() Note by default, geom_col takes both x and y while geom_bar takes only x and plots the count on y. Just as before, we can add error bars: ggplot(data = subset(df, year == 1977), aes(x=continent, y=mean_le)) + geom_col() + geom_errorbar(aes(ymin = mean_le - sem, ymax = mean_le + sem), width=0.5, position=position_dodge(0.05)) We can compare 1977 and 2007 by settingfill = as.factor(year). p &lt;- ggplot(data = subset(df, year %in% c(1977, 2007)), aes(x=continent, y=mean_le, fill = as.factor(year))) + geom_col(position = position_dodge(), color = &quot;black&quot;) + geom_errorbar(aes(ymin = mean_le - sem, ymax = mean_le + sem), width=0.5, position=position_dodge(0.9)) show(p) Finally, let’s make this figure publication-ready: p &lt;- p + scale_fill_manual(values = c(&#39;black&#39;, &#39;white&#39;)) + labs(x = &#39;Continent&#39;, y=&#39;Mean life expectancy (years)&#39;, fill = &#39;Year&#39;) + theme_classic() + theme(text = element_text(size=16)) + # removes space between bottom of bars and x-axis scale_y_continuous(expand = c(0, 0)) p Plotting significance (OPTIONAL) Next we can run some statistical tests and add significance stars (*) to the plot. We’ll compare if the life expectancy in Africa in 1977 and 2007 has changed. africa.1977.lifeExp &lt;- gapminder %&gt;% filter(continent == &#39;Africa&#39;, year == 1977) %&gt;% select(lifeExp) africa.2007.lifeExp &lt;- gapminder %&gt;% filter(continent == &#39;Africa&#39;, year == 2007) %&gt;% select(lifeExp) t.test(africa.1977.lifeExp, africa.2007.lifeExp) ## ## Welch Two Sample t-test ## ## data: africa.1977.lifeExp and africa.2007.lifeExp ## t = -3.195, df = 91.787, p-value = 0.001917 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -8.474086 -1.977145 ## sample estimates: ## mean of x mean of y ## 49.58042 54.80604 The p-value is 0.001917. Here are common ranges for different p-values: 0 **** 0.0001 *** 0.001 ** 0.01 * 0.05 ns 1. We should use ** in this case. To draw the significance stars, we need the ggsignif package. # install.packages(&quot;ggsignif&quot;) library(ggsignif) meanLE.77.07 &lt;- p + geom_signif(y_position = 60, xmin = 0.75, xmax = 1.25, # position of the stars annotations = &quot;**&quot;, tip_length = 0.05) meanLE.77.07 There are more ways to draw the significant stars in R. For example, ggpubr even allows you to run the tests and plot the stars in the same line. However, I do not encourage you to do so. Manually adding the stars might be a bit tedious (in terms of adjusting the positions and tip length), but you are not as restricted by the package in terms of the tests you can do. 14.3 Box plot Box plots are extremely versatile. Here are 2 reasons: You don’t need to calculate the mean and error for box plots (remember we used df for bar plots). Many high-profile journals ask authors to submit graphs that shows not only the statistical description (mean and error), but also the dots for the raw data. Box plots are well suited for this purpose. p &lt;- ggplot(data = subset(gapminder, year %in% c(1977, 2007)), aes(x = continent, y = lifeExp, color = as.factor(year))) + geom_boxplot(position = position_dodge(0.8)) show(p) Let’s add the data points. Here I use geom_jitter to avoid overlapping. Please note that position_jitterdodge introduces random noise to the x position of the points to make it easier to read. But since we are plotting against a categorical variable, the exact x position doesn’t matter. p &lt;- p + geom_point(position = position_jitterdodge(0.4), alpha = 0.65) Use stat_summary show data points and statistics together. p &lt;- ggplot(data = subset(gapminder, year %in% c(1977, 2007)), aes(x = continent, y = lifeExp, color = as.factor(year))) + geom_point(position = position_jitterdodge(0.4), alpha = 0.65) + theme_classic() + # adds error bar stat_summary(fun.data=mean_sdl, fun.args = list(mult=1), geom=&quot;errorbar&quot;, width=0.3, position = position_dodge(0.7), size = 1) + # adds mean pooint to the error bar stat_summary(fun=mean, geom=&quot;point&quot;, position = position_dodge(0.7), size = 3) show(p) Add asterisks and format the plot. p &lt;- p + geom_signif(y_position = 85, xmin = 0.75, xmax = 1.25, annotations = &quot;**&quot;, tip_length = 0.02, color = &quot;black&quot;) + labs(x = &quot;Continents&quot;, y = &quot;Life Expectancy (years)&quot;, color = &quot;Year&quot;) + theme(text = element_text(size = 16)) # make text larger show(p) ## Warning: Computation failed in `stat_summary()`: ## Hmisc package required for this function We can again offset the axes. offset_box &lt;- p + theme(axis.line = element_blank(), # hide the default axes axis.ticks.length = unit(7, &#39;pt&#39;)) + # Specify the breaks of y-axis scale_y_continuous(breaks=seq(30,90,30), limits=c(25,95), expand=c(0,0)) + # Specify location of x-axis geom_segment(y=30, yend=90, x=0.4, xend=0.4, lwd=0.5, colour=&quot;black&quot;, lineend=&quot;square&quot;) + # Specify location of y-axis geom_segment(y=25, yend=25, x=1, xend=5, lwd=0.5, colour=&quot;black&quot;, lineend=&quot;square&quot;) show(offset_box) ## Warning: Computation failed in `stat_summary()`: ## Hmisc package required for this function Now we can save this publication-ready figure. ggsave(&#39;data/10_pubvis/life_year_jitter.png&#39;, plot = offset_box) ## Saving 7 x 5 in image ## Warning: Computation failed in `stat_summary()`: ## Hmisc package required for this function 14.4 Histogram and density plot Since these have been covered extensively in previous labs, I’m going to go through this section rather quickly. The histogram and density plot are great tools for looking at distributions of a single variable: ggplot(data = subset(gapminder, year == 2007), aes(x = lifeExp)) + geom_histogram(fill = &#39;#69B3A2&#39;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ggplot(data = subset(gapminder, year == 2007), aes(x = lifeExp)) + geom_density(fill = &#39;#69B3A2&#39;) We can also use these plots to compare distributions. ggplot() + # 2007 data and label geom_density(data = subset(gapminder, year == 2007), aes(x = lifeExp, y = ..density..), fill = &#39;#69B3A2&#39;) + geom_label(aes(x=40, y=0.03, label=&#39;2007&#39;), colour = &#39;#69B3A2&#39;) + # 1977 data and label geom_density(data = subset(gapminder, year == 1977), aes(x = lifeExp, y = -..density..), fill = &#39;#404080&#39;) + geom_label(aes(x=40, y=-0.03, label=&#39;1977&#39;), colour = &#39;#404080&#39;) + theme_minimal() Perhaps a histogram would be better for our purposes. hist.07.77 &lt;- ggplot() + # 2007 data and label geom_histogram(data = subset(gapminder, year == 2007), aes(x = lifeExp, y = ..density..), fill = &#39;#69B3A2&#39;) + geom_label(aes(x=40, y=0.03, label=&#39;2007&#39;), colour = &#39;#69B3A2&#39;) + # 1977 data and label geom_histogram(data = subset(gapminder, year == 1977), aes(x = lifeExp, y = -..density..), fill = &#39;#404080&#39;) + geom_label(aes(x=40, y=-0.03, label=&#39;1977&#39;), colour = &#39;#404080&#39;) + theme_minimal() hist.07.77 ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Assembly of multiple figures (OPTIONAL) Many figures in academic journals include multiple subfigures within a figure. To assemble many graphs into one figure, usually we use a graphical design software such as Illustrator, Inkscape or even PowerPoint. If you are able to generate all subfigures within one single R script (so that all the ggplot2 objects are present together), you could go on to use R package patchwork to assemble them into a big figure. (Please note that this could be a very rare scenario when conducting serious research - Each subfigures may require intense computation and modelling work that are performed with several scripts. They may even come from different people - your teammates and collaborators. You don’t always have access to all te subfigures within one workspace. Most of the time you would still find yourself using graphical design softwares to assemble the figures.) # install.packages(&#39;patchwork&#39;) library(patchwork) Here is a super simple example: just add the plots together! Recall that these variables were saved throughout our lab. hist.07.77 + lineplot ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. We can also do something a bit more complicated: (hist.07.77 + lineplot) / offset_box ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Computation failed in `stat_summary()`: ## Hmisc package required for this function We can also add spacing between plots: patch &lt;- (hist.07.77 + plot_spacer() + lineplot) / (meanLE.77.07 + theme(text=element_text(size=12))) patch ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Most figures in papers are annotated. In most cases, plots are combined using a photoshop tool. Labels are usually also added with a photoshop tool. For whatever reason you wish to programmatically add labels, here’s howyou do it: (hist.07.77 + lineplot) / (meanLE.77.07 + theme(text=element_text(size=12))) + plot_annotation(tag_levels = &#39;A&#39;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Note that tag_levels takes: ‘1’ for Arabic numerals, ‘A’ for uppercase Latin letters, ‘a’ for lowercase Latin letters, ‘I’ for uppercase Roman numerals, and ‘i’ for lowercase Roman numerals. More often than not, we want all of our figures to have the same dimensions. patchwork makes it easy for us to do so: aligned_plots &lt;- align_patches(lineplot, scatter_trend, meanLE.77.07, offset_box) for (p in aligned_plots) { plot(p) } That’s all there is to it! If you want more customization options, read the official documentation: https://patchwork.data-imaginist.com/articles/patchwork.html. Extras (OPTIONAL) Choropleth “The greatest value of a picture is when it forces us to notice what we never expected to see.” –John Tukey The choropleth is used to display differences in geographical regions using different colours/shades/patterns. To use map data, we need to install maps package. install.packages(&quot;maps&quot;) First, let’s load maps and retrieve all of the data from the year 2007. library(maps) dat2007 &lt;- gapminder %&gt;% filter(year == 2007) dat2007 &lt;- dat2007 %&gt;% rename(&#39;region&#39; = &#39;country&#39;) Now let’s get the world map data. This is necessary because it contains the longitude and latitudes we need to draw the map world_map &lt;- map_data(&quot;world&quot;) head(world_map) ## long lat group order region subregion ## 1 -69.89912 12.45200 1 1 Aruba &lt;NA&gt; ## 2 -69.89571 12.42300 1 2 Aruba &lt;NA&gt; ## 3 -69.94219 12.43853 1 3 Aruba &lt;NA&gt; ## 4 -70.00415 12.50049 1 4 Aruba &lt;NA&gt; ## 5 -70.06612 12.54697 1 5 Aruba &lt;NA&gt; ## 6 -70.05088 12.59707 1 6 Aruba &lt;NA&gt; Whereas gapminder names USA and UK ‘United States’ and ‘United Kingdom’ respectively, world_map names them by their abbreviations. Let’s rename the gapminder data. dat2007 &lt;- dat2007 %&gt;% mutate(region = fct_recode(region, &#39;USA&#39; = &#39;United States&#39;, &#39;UK&#39; = &#39;United Kingdom&#39;)) We’re almost there! Now, we need to merge the data. life.exp.map &lt;- left_join(world_map, dat2007, by = &quot;region&quot;) head(life.exp.map) ## long lat group order region subregion continent year lifeExp pop ## 1 -69.89912 12.45200 1 1 Aruba &lt;NA&gt; &lt;NA&gt; NA NA NA ## 2 -69.89571 12.42300 1 2 Aruba &lt;NA&gt; &lt;NA&gt; NA NA NA ## 3 -69.94219 12.43853 1 3 Aruba &lt;NA&gt; &lt;NA&gt; NA NA NA ## 4 -70.00415 12.50049 1 4 Aruba &lt;NA&gt; &lt;NA&gt; NA NA NA ## 5 -70.06612 12.54697 1 5 Aruba &lt;NA&gt; &lt;NA&gt; NA NA NA ## 6 -70.05088 12.59707 1 6 Aruba &lt;NA&gt; &lt;NA&gt; NA NA NA ## gdpPercap ## 1 NA ## 2 NA ## 3 NA ## 4 NA ## 5 NA ## 6 NA Finally, we can plot the data. We specify group to draw each country individually. # grey means no gapminder data ggplot(life.exp.map, aes(x=long, y=lat, group = group)) + geom_polygon(aes(fill = lifeExp), color = &quot;white&quot;) + scale_fill_viridis_b(option=&#39;D&#39;) + theme_bw() Animations While we covered the most relevant data visualizations for your project, we’ve barely scratched the surface of what R can do. For example, you can create animations. Let’s install the gganimate package. install.packages(&#39;devtools&#39;) devtools::install_github(&#39;thomasp85/gganimate&#39;) Next, load the package: library(gganimate) Finally, we can create the plot! This might take a while, but I promise it will be worth the wait! ggplot(gapminder, aes(gdpPercap, lifeExp, size = pop, col = country)) + geom_point(alpha = 0.7, show.legend = F) + scale_colour_manual(values = country_colors) + scale_x_log10() + scale_size(range = c(2, 12)) + facet_wrap(~continent) + theme_bw() + theme(panel.grid = element_blank()) + # here is the animation code labs(title = &#39;Year: {frame_time}&#39;, x = &#39;GDP per capita&#39;, y = &#39;Life expectancy (years)&#39;) + transition_time(year) + ease_aes(&#39;linear&#39;) Additional resources I highly recommend you read through these websites: From Data to Viz: https://www.data-to-viz.com Includes the visualization and the type of data it corresponds to. Patchwork: https://github.com/thomasp85/patchwork For creating multi-plot figures. Caveats: https://www.data-to-viz.com/caveats.html Pitfalls to avooid when creating figures. The Python Graph Gallery: https://python-graph-gallery.com If you’re more comfortable with Python. Includes the visualization and code to create it. Data visualizations for various data types. Check out https://www.data-to-viz.com/ for an interactive version of this chart! "]]
